{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henry/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2205077/2717389824.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict, config = torch.load(SAE_PATH, map_location=device).values()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SAE with 65536 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.04s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.70it/s]\n",
      "Some weights of the model checkpoint at allenai/OLMo-2-1124-7B-Instruct were not used when initializing Olmo2ForCausalLM: ['model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.16.post_feedforward_layernorm.weight', 'model.layers.16.self_attn.k_norm.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_norm.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.17.post_feedforward_layernorm.weight', 'model.layers.17.self_attn.k_norm.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_norm.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.18.post_feedforward_layernorm.weight', 'model.layers.18.self_attn.k_norm.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_norm.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.19.post_feedforward_layernorm.weight', 'model.layers.19.self_attn.k_norm.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_norm.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.20.post_feedforward_layernorm.weight', 'model.layers.20.self_attn.k_norm.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_norm.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.21.post_feedforward_layernorm.weight', 'model.layers.21.self_attn.k_norm.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_norm.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.22.post_feedforward_layernorm.weight', 'model.layers.22.self_attn.k_norm.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_norm.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.23.post_feedforward_layernorm.weight', 'model.layers.23.self_attn.k_norm.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_norm.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.24.post_feedforward_layernorm.weight', 'model.layers.24.self_attn.k_norm.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_norm.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.25.post_feedforward_layernorm.weight', 'model.layers.25.self_attn.k_norm.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_norm.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.26.post_feedforward_layernorm.weight', 'model.layers.26.self_attn.k_norm.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_norm.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.27.post_feedforward_layernorm.weight', 'model.layers.27.self_attn.k_norm.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_norm.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.28.post_feedforward_layernorm.weight', 'model.layers.28.self_attn.k_norm.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_norm.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.29.post_feedforward_layernorm.weight', 'model.layers.29.self_attn.k_norm.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_norm.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.30.post_feedforward_layernorm.weight', 'model.layers.30.self_attn.k_norm.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_norm.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.31.post_feedforward_layernorm.weight', 'model.layers.31.self_attn.k_norm.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_norm.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight']\n",
      "- This IS expected if you are initializing Olmo2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Olmo2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 39999744 tokens (2228.3 tok/s)\n",
      "Processed 40000000 tokens in 17951.12s\n",
      "Dead features: 21834 / 65536 (33.32%)\n",
      "\n",
      "\n",
      "--- Profiling Results ---\n",
      "         11274669237 function calls (11232668199 primitive calls) in 17826.901 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 14472 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    3.715    3.715 18038.575 18038.575 /tmp/ipykernel_2205077/2717389824.py:66(main)\n",
      "        1 9835.364 9835.364 17958.598 17958.598 /tmp/ipykernel_2205077/2717389824.py:101(mine_top_tokens_and_dead)\n",
      "   468751 5528.595    0.012 5528.595    0.012 {method 'cpu' of 'torch._C.TensorBase' objects}\n",
      "   156251   11.797    0.000 1946.550    0.012 /tmp/ipykernel_2205077/2717389824.py:89(residual_stream_iter)\n",
      "6094293/312500   11.743    0.000 1512.591    0.005 /home/henry/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1549(_wrapped_call_impl)\n",
      "6094293/312500  144.188    0.000 1511.623    0.005 /home/henry/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1555(_call_impl)\n",
      "   156250    2.255    0.000 1489.849    0.010 /home/henry/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:399(_fn)\n",
      "312500/156250   10.361    0.000 1482.781    0.009 /home/henry/.local/lib/python3.10/site-packages/transformers/utils/generic.py:953(wrapper)\n",
      "   156250    3.961    0.000 1470.573    0.009 /home/henry/.local/lib/python3.10/site-packages/transformers/models/olmo2/modeling_olmo2.py:622(forward)\n",
      "   156250   67.828    0.000 1428.410    0.009 /home/henry/.local/lib/python3.10/site-packages/transformers/models/olmo2/modeling_olmo2.py:366(forward)\n",
      "8281359/8281305   14.041    0.000  931.065    0.000 /home/henry/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:596(_fn)\n",
      "  8125000   15.250    0.000  904.892    0.000 /home/henry/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:983(forward)\n",
      "  8125000   47.348    0.000  885.003    0.000 /home/henry/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:185(runtime_wrapper)\n",
      "  8125000   27.461    0.000  827.907    0.000 /home/henry/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:110(call_func_at_runtime_with_args)\n",
      "  8125000   10.463    0.000  783.163    0.000 /home/henry/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:437(wrapper)\n",
      "  8125000   10.438    0.000  772.701    0.000 /home/henry/.local/lib/python3.10/site-packages/torch/_inductor/codecache.py:1129(__call__)\n",
      "  8125000    8.490    0.000  762.263    0.000 /home/henry/.local/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:988(run)\n",
      "  8125000   13.924    0.000  753.763    0.000 /home/henry/.local/lib/python3.10/site-packages/torch/_inductor/cudagraph_trees.py:356(deferred_cudagraphify)\n",
      "  8124948   16.486    0.000  734.920    0.000 /home/henry/.local/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:942(run)\n",
      "8281247/8125000   14.949    0.000  696.825    0.000 /home/henry/.local/lib/python3.10/site-packages/torch/_inductor/cudagraph_trees.py:1839(run)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Profiling Results by Function Calls ---\n",
      "         11274669237 function calls (11232668199 primitive calls) in 17826.901 seconds\n",
      "\n",
      "   Ordered by: call count\n",
      "   List reduced from 14472 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "10252298602/10252127595  556.260    0.000  556.391    0.000 {built-in method builtins.len}\n",
      "106561179/106441487   13.318    0.000   14.807    0.000 {built-in method builtins.isinstance}\n",
      "58836734/58682152    8.015    0.000    8.092    0.000 {built-in method builtins.hash}\n",
      " 41563256    5.051    0.000    5.051    0.000 /home/henry/.local/lib/python3.10/site-packages/torch/_inductor/cudagraph_trees.py:2103(current_node)\n",
      "39583750/13238278   16.863    0.000   22.086    0.000 /usr/lib/python3.10/json/encoder.py:277(_iterencode_list)\n",
      " 32725717   15.283    0.000   15.283    0.000 /home/henry/.local/lib/python3.10/site-packages/torch/utils/_config_module.py:141(__getattr__)\n",
      " 25479943   45.199    0.000   45.199    0.000 {built-in method _heapq.heapreplace}\n",
      " 24536971   17.983    0.000   29.141    0.000 /home/henry/.local/lib/python3.10/site-packages/torch/utils/_config_module.py:370(__getattr__)\n",
      " 23126120    7.792    0.000    7.792    0.000 {method 'data_ptr' of 'torch._C.TensorBase' objects}\n",
      " 20533235    2.754    0.000    2.754    0.000 {method 'append' of 'list' objects}\n",
      " 16875218    3.943    0.000    3.943    0.000 {built-in method torch._C._dynamo.eval_frame.set_eval_frame}\n",
      " 16718742    6.306    0.000    6.306    0.000 /home/henry/.local/lib/python3.10/site-packages/torch/_inductor/cudagraph_trees.py:2091(in_recording)\n",
      " 16562442    4.811    0.000    4.811    0.000 /home/henry/.local/lib/python3.10/site-packages/torch/_inductor/cudagraph_trees.py:2095(in_warmup)\n",
      " 16383787    5.579    0.000    5.684    0.000 {method 'extend' of 'list' objects}\n",
      " 16249792   27.373    0.000   27.373    0.000 {built-in method torch._C._tensors_data_ptrs_at_indices_equal}\n",
      " 16249792   14.314    0.000   58.681    0.000 /home/henry/.local/lib/python3.10/site-packages/torch/_inductor/cudagraph_trees.py:1372(_check_liveness)\n",
      "13578753/13542781    9.138    0.000   13.593    0.000 {built-in method builtins.hasattr}\n",
      " 13238480    3.003    0.000   25.089    0.000 /usr/lib/python3.10/json/encoder.py:413(_iterencode)\n",
      " 13238273    1.412    0.000    1.412    0.000 {method 'write' of '_io.TextIOWrapper' objects}\n",
      " 10651291    5.334    0.000    5.334    0.000 {built-in method torch._C._storage_Use_Count}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import heapq\n",
    "import pickle\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cProfile\n",
    "import pstats\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from psutil import cpu_count\n",
    "\n",
    "SAE_PATH      = Path('out/sae_65k_lambda_ramp_80to25/sae_final.pt')\n",
    "TOP_N         = 50\n",
    "DUMP_DIR      = Path('feature_dumps')\n",
    "DUMP_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n",
    "# ---- Load SAE (replace with your own class/loader) ----\n",
    "from model import SAE\n",
    "state_dict, config = torch.load(SAE_PATH, map_location=device).values()\n",
    "sae = SAE(config['input_size'],config['hidden_size']).to(device).to(torch.bfloat16)\n",
    "# Fix for \"_orig_mod\" prefix in state dict keys\n",
    "fixed_state_dict = {k.replace('_orig_mod.', ''): v for k, v in state_dict.items()}\n",
    "sae.load_state_dict(fixed_state_dict)\n",
    "\n",
    "n_features = sae.encode.out_features if hasattr(sae.encode,'out_features') else sae.n_features\n",
    "print(f'Loaded SAE with {n_features} features')\n",
    "\n",
    "def count_dead_features(sample_iter, sample_tokens=10_000_000):\n",
    "    \"\"\"Returns a boolean tensor of shape (n_features,) where True == dead.\"\"\"\n",
    "    fired = torch.zeros(n_features, dtype=torch.bool, device=device)\n",
    "    seen  = 0\n",
    "    for toks in tqdm(sample_iter, total=sample_tokens//len(next(iter(sample_iter)))):\n",
    "        toks = toks.to(device)\n",
    "        acts = sae.encode(toks) > 0  # bool mask of activations\n",
    "        fired |= acts.any(dim=0)\n",
    "        seen  += toks.size(0)\n",
    "        if seen >= sample_tokens:\n",
    "            break\n",
    "    dead_mask = ~fired.cpu()\n",
    "    print(f\"Dead features: {dead_mask.sum().item()} / {n_features} ({dead_mask.float().mean()*100:.2f}%)\")\n",
    "    return dead_mask\n",
    "\n",
    "# GPU optimizations\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# Constants\n",
    "MODEL_NAME = \"allenai/OLMo-2-1124-7B-Instruct\"\n",
    "BATCH_SIZE = 256\n",
    "LAYER_OFFSET = -1\n",
    "TOP_N = 50\n",
    "device = \"cuda\"\n",
    "DUMP_DIR = Path(\"./results\")\n",
    "os.makedirs(DUMP_DIR, exist_ok=True)\n",
    "\n",
    "def main():\n",
    "    # Load tokenizer + *half* model config (bf16, compiled)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    cfg = AutoModelForCausalLM.from_pretrained(MODEL_NAME).config\n",
    "    cfg.num_hidden_layers //= 2  # half-model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        config=cfg,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        attn_implementation=\"sdpa\",\n",
    "        device_map=device\n",
    "    ).eval()\n",
    "    model = torch.compile(model, mode=\"reduce-overhead\")\n",
    "\n",
    "    # Load dataset\n",
    "    dataset_iter = load_dataset(\n",
    "        \"HuggingFaceFW/fineweb\",\n",
    "        name=\"sample-10BT\",\n",
    "        split=\"train\",\n",
    "        streaming=False,\n",
    "        num_proc=cpu_count(),\n",
    "    ).shuffle()\n",
    "\n",
    "    def residual_stream_iter(text_iter, batch=BATCH_SIZE):\n",
    "        buf = []\n",
    "        for record in text_iter:\n",
    "            buf.extend(tokenizer(record[\"text\"]).input_ids)\n",
    "            while len(buf) >= batch:\n",
    "                toks = torch.tensor(buf[:batch]).to(device)\n",
    "                buf = buf[batch:]\n",
    "                with torch.inference_mode():\n",
    "                    outs = model(toks.unsqueeze(0), output_hidden_states=True)\n",
    "                    resid = outs.hidden_states[LAYER_OFFSET].squeeze(0)  # (T, d)\n",
    "                yield toks, resid  # feed straight to SAE\n",
    "\n",
    "    def mine_top_tokens_and_dead(data_iter,\n",
    "                              top_n=TOP_N,\n",
    "                              target_tokens=100_000):\n",
    "        \"\"\"\n",
    "        • Keeps the TOP-N strongest (activation, token) pairs per feature.\n",
    "        • Tracks which features ever fire to flag the 'dead' ones.\n",
    "        • Stops after `target_tokens` have been processed.\n",
    "        \"\"\"\n",
    "        n_features = sae.encode.weight.shape[0]\n",
    "        \n",
    "        # Pre-allocate all buckets with empty heaps\n",
    "        buckets = [[] for _ in range(n_features)]\n",
    "        fired = torch.zeros(n_features, dtype=torch.bool, device=device)\n",
    "        seen_toks = 0\n",
    "        \n",
    "        # Process batches with lighter progress indicator \n",
    "        start_time = time.time()\n",
    "        batch_count = 0\n",
    "        \n",
    "        for toks, resid in data_iter:\n",
    "            batch_count += 1\n",
    "            if batch_count % 10 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                tokens_per_sec = seen_toks / elapsed if elapsed > 0 else 0\n",
    "                print(f\"\\rProcessed {seen_toks} tokens ({tokens_per_sec:.1f} tok/s)\", end=\"\")\n",
    "                \n",
    "            with torch.inference_mode():\n",
    "                # Compute activations\n",
    "                acts = sae.encode(resid)\n",
    "                fired |= (acts > 0).any(dim=0)\n",
    "                \n",
    "                # Get top values and indices\n",
    "                values, idx = acts.topk(1, dim=0)\n",
    "                \n",
    "                # Transfer to CPU in one batch  \n",
    "                values_cpu = values[0].to(torch.float32).detach().cpu().numpy()\n",
    "                indices_cpu = idx[0].detach().cpu().numpy()\n",
    "                token_ids = toks[indices_cpu].cpu().numpy()\n",
    "                \n",
    "                # Process features in chunks for better performance\n",
    "                for f in range(n_features):\n",
    "                    val, tok_id = float(values_cpu[f]), int(token_ids[f])\n",
    "                    heap = buckets[f]\n",
    "                    if len(heap) < top_n:\n",
    "                        heapq.heappush(heap, (val, tok_id))\n",
    "                    elif val > heap[0][0]:\n",
    "                        heapq.heapreplace(heap, (val, tok_id))\n",
    "            \n",
    "            seen_toks += toks.numel()\n",
    "            if seen_toks >= target_tokens:\n",
    "                break\n",
    "        \n",
    "        print(f\"\\nProcessed {seen_toks} tokens in {time.time() - start_time:.2f}s\")\n",
    "        \n",
    "        # Post-process\n",
    "        dead_mask = ~fired.cpu().numpy()  # Convert directly to numpy\n",
    "        \n",
    "        # Get unique token IDs for batch decoding\n",
    "        unique_token_ids = set()\n",
    "        for heap in buckets:\n",
    "            for _, tok_id in heap:\n",
    "                unique_token_ids.add(tok_id)\n",
    "        \n",
    "        # Convert set to list for batch decoding\n",
    "        unique_token_list = list(unique_token_ids)\n",
    "        decoded_tokens = tokenizer.batch_decode([[t] for t in unique_token_list])\n",
    "        \n",
    "        # Create mapping of token ID to decoded text\n",
    "        token_id_to_text = {unique_token_list[i]: decoded_tokens[i] for i in range(len(unique_token_list))}\n",
    "        \n",
    "        # Create the final result with native Python types\n",
    "        decoded = []\n",
    "        for heap in buckets:\n",
    "            feature_results = []\n",
    "            for val, tok_id in sorted(heap, key=lambda x: -x[0]):  # Sort directly here\n",
    "                feature_results.append((float(val), token_id_to_text[tok_id]))\n",
    "            decoded.append(feature_results)  # Already sorted\n",
    "        \n",
    "        print(f\"Dead features: {dead_mask.sum()} / {n_features} \"\n",
    "              f\"({dead_mask.sum()/n_features*100:.2f}%)\")\n",
    "        \n",
    "        return decoded, dead_mask\n",
    "\n",
    "    data_iter = residual_stream_iter(dataset_iter)\n",
    "    with torch.inference_mode():\n",
    "        top_buckets, dead_mask = mine_top_tokens_and_dead(\n",
    "            data_iter,\n",
    "            top_n=50,\n",
    "            target_tokens=40_000_000\n",
    "        )\n",
    "\n",
    "    # Save results - using faster formats where possible\n",
    "    # Use pickle for faster serialization\n",
    "    with open(DUMP_DIR / \"top_tokens.pkl\", \"wb\") as f:\n",
    "        pickle.dump(top_buckets, f)\n",
    "    \n",
    "    with open(DUMP_DIR / \"top_tokens.json\", \"w\") as f:\n",
    "        json.dump(top_buckets, f)\n",
    "    \n",
    "    np.save(DUMP_DIR / \"dead_features.npy\", dead_mask)\n",
    "    \n",
    "    pd.Series(dead_mask).to_csv(DUMP_DIR / \"dead_features.csv\", index=False)\n",
    "\n",
    "# Run with profiling\n",
    "if __name__ == \"__main__\":\n",
    "    profiler = cProfile.Profile()\n",
    "    profiler.enable()\n",
    "    \n",
    "    main()\n",
    "    \n",
    "    profiler.disable()\n",
    "    \n",
    "    # Save stats to a file\n",
    "    stats = pstats.Stats(profiler)\n",
    "    stats.sort_stats('cumtime')\n",
    "    stats.dump_stats('profile_results.prof')\n",
    "    \n",
    "    print(\"\\n\\n--- Profiling Results ---\")\n",
    "    stats.sort_stats('cumtime').print_stats(20)\n",
    "    \n",
    "    print(\"\\n\\n--- Profiling Results by Function Calls ---\")\n",
    "    stats.sort_stats('calls').print_stats(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def auto_label_feature(token_list):\n",
    "    \"\"\"Return a one‑line label via OpenAI LLM (adjust model & key).\"\"\"\n",
    "    prompt = (\n",
    "        'Tokens that all activate the same hidden feature:\\n' +\n",
    "        ', '.join(token_list) +\n",
    "        '\\n\\nGive a concise, literal description of what these tokens have in common. '\n",
    "        'If unsure, reply \"uncertain\".'\n",
    "    )\n",
    "    client = OpenAI()\n",
    "    resp = client.chat.completions.create(model='gpt-4o-mini', messages=[{'role':'user','content':prompt}])\n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "# Example:\n",
    "# labels = [auto_label_feature([tok for _, tok in bucket]) for bucket in top_buckets]\n",
    "# pd.Series(labels).to_csv(DUMP_DIR/'auto_labels.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
