{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import heapq\n",
    "import pickle\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cProfile\n",
    "import pstats\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from psutil import cpu_count\n",
    "\n",
    "SAE_PATH      = Path('out/sae_65k_lambda26_ramp30/sae_final.pt')\n",
    "TOP_N         = 50\n",
    "DUMP_DIR      = Path('feature_dumps')\n",
    "DUMP_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n",
    "# ---- Load SAE (replace with your own class/loader) ----\n",
    "from model import SAE\n",
    "state_dict, config = torch.load(SAE_PATH, map_location=device).values()\n",
    "sae = SAE(config['input_size'],config['hidden_size']).to(device).to(torch.bfloat16)\n",
    "# Fix for \"_orig_mod\" prefix in state dict keys\n",
    "fixed_state_dict = {k.replace('_orig_mod.', ''): v for k, v in state_dict.items()}\n",
    "sae.load_state_dict(fixed_state_dict)\n",
    "\n",
    "n_features = sae.encode.out_features if hasattr(sae.encode,'out_features') else sae.n_features\n",
    "print(f'Loaded SAE with {n_features} features')\n",
    "\n",
    "def count_dead_features(sample_iter, sample_tokens=10_000_000):\n",
    "    \"\"\"Returns a boolean tensor of shape (n_features,) where True == dead.\"\"\"\n",
    "    fired = torch.zeros(n_features, dtype=torch.bool, device=device)\n",
    "    seen  = 0\n",
    "    for toks in tqdm(sample_iter, total=sample_tokens//len(next(iter(sample_iter)))):\n",
    "        toks = toks.to(device)\n",
    "        acts = sae.encode(toks) > 0  # bool mask of activations\n",
    "        fired |= acts.any(dim=0)\n",
    "        seen  += toks.size(0)\n",
    "        if seen >= sample_tokens:\n",
    "            break\n",
    "    dead_mask = ~fired.cpu()\n",
    "    print(f\"Dead features: {dead_mask.sum().item()} / {n_features} ({dead_mask.float().mean()*100:.2f}%)\")\n",
    "    return dead_mask\n",
    "\n",
    "# GPU optimizations\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# Constants\n",
    "MODEL_NAME = \"allenai/OLMo-2-1124-7B-Instruct\"\n",
    "BATCH_SIZE = 256\n",
    "LAYER_OFFSET = -1\n",
    "TOP_N = 50\n",
    "device = \"cuda\"\n",
    "DUMP_DIR = Path(\"./results\")\n",
    "os.makedirs(DUMP_DIR, exist_ok=True)\n",
    "\n",
    "def main():\n",
    "    # Load tokenizer + *half* model config (bf16, compiled)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    cfg = AutoModelForCausalLM.from_pretrained(MODEL_NAME).config\n",
    "    cfg.num_hidden_layers //= 2  # half-model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        config=cfg,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        attn_implementation=\"sdpa\",\n",
    "        device_map=device\n",
    "    ).eval()\n",
    "    model = torch.compile(model, mode=\"reduce-overhead\")\n",
    "\n",
    "    # Load dataset\n",
    "    dataset_iter = load_dataset(\n",
    "        \"HuggingFaceFW/fineweb\",\n",
    "        name=\"sample-10BT\",\n",
    "        split=\"train\",\n",
    "        streaming=False,\n",
    "        num_proc=cpu_count(),\n",
    "    ).shuffle()\n",
    "\n",
    "    def residual_stream_iter(text_iter, batch=BATCH_SIZE):\n",
    "        buf = []\n",
    "        for record in text_iter:\n",
    "            buf.extend(tokenizer(record[\"text\"]).input_ids)\n",
    "            while len(buf) >= batch:\n",
    "                toks = torch.tensor(buf[:batch]).to(device)\n",
    "                buf = buf[batch:]\n",
    "                with torch.inference_mode():\n",
    "                    outs = model(toks.unsqueeze(0), output_hidden_states=True)\n",
    "                    resid = outs.hidden_states[LAYER_OFFSET].squeeze(0)  # (T, d)\n",
    "                yield toks, resid  # feed straight to SAE\n",
    "\n",
    "    def mine_top_tokens_and_dead(data_iter,\n",
    "                              top_n=TOP_N,\n",
    "                              target_tokens=100_000):\n",
    "        \"\"\"\n",
    "        • Keeps the TOP-N strongest (activation, token) pairs per feature.\n",
    "        • Tracks which features ever fire to flag the 'dead' ones.\n",
    "        • Stops after `target_tokens` have been processed.\n",
    "        \"\"\"\n",
    "        n_features = sae.encode.weight.shape[0]\n",
    "        \n",
    "        # Pre-allocate all buckets with empty heaps\n",
    "        buckets = [[] for _ in range(n_features)]\n",
    "        fired = torch.zeros(n_features, dtype=torch.bool, device=device)\n",
    "        seen_toks = 0\n",
    "        \n",
    "        # Process batches with lighter progress indicator \n",
    "        start_time = time.time()\n",
    "        batch_count = 0\n",
    "        \n",
    "        for toks, resid in data_iter:\n",
    "            batch_count += 1\n",
    "            if batch_count % 10 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                tokens_per_sec = seen_toks / elapsed if elapsed > 0 else 0\n",
    "                print(f\"\\rProcessed {seen_toks} tokens ({tokens_per_sec:.1f} tok/s)\", end=\"\")\n",
    "                \n",
    "            with torch.inference_mode():\n",
    "                # Compute activations\n",
    "                acts = sae.encode(resid)\n",
    "                fired |= (acts > 0).any(dim=0)\n",
    "                \n",
    "                # Get top values and indices\n",
    "                values, idx = acts.topk(1, dim=0)\n",
    "                \n",
    "                # Transfer to CPU in one batch  \n",
    "                values_cpu = values[0].to(torch.float32).detach().cpu().numpy()\n",
    "                indices_cpu = idx[0].detach().cpu().numpy()\n",
    "                token_ids = toks[indices_cpu].cpu().numpy()\n",
    "                \n",
    "                # Process features in chunks for better performance\n",
    "                for f in range(n_features):\n",
    "                    val, tok_id = float(values_cpu[f]), int(token_ids[f])\n",
    "                    heap = buckets[f]\n",
    "                    if len(heap) < top_n:\n",
    "                        heapq.heappush(heap, (val, tok_id))\n",
    "                    elif val > heap[0][0]:\n",
    "                        heapq.heapreplace(heap, (val, tok_id))\n",
    "            \n",
    "            seen_toks += toks.numel()\n",
    "            if seen_toks >= target_tokens:\n",
    "                break\n",
    "        \n",
    "        print(f\"\\nProcessed {seen_toks} tokens in {time.time() - start_time:.2f}s\")\n",
    "        \n",
    "        # Post-process\n",
    "        dead_mask = ~fired.cpu().numpy()  # Convert directly to numpy\n",
    "        \n",
    "        # Get unique token IDs for batch decoding\n",
    "        unique_token_ids = set()\n",
    "        for heap in buckets:\n",
    "            for _, tok_id in heap:\n",
    "                unique_token_ids.add(tok_id)\n",
    "        \n",
    "        # Convert set to list for batch decoding\n",
    "        unique_token_list = list(unique_token_ids)\n",
    "        decoded_tokens = tokenizer.batch_decode([[t] for t in unique_token_list])\n",
    "        \n",
    "        # Create mapping of token ID to decoded text\n",
    "        token_id_to_text = {unique_token_list[i]: decoded_tokens[i] for i in range(len(unique_token_list))}\n",
    "        \n",
    "        # Create the final result with native Python types\n",
    "        decoded = []\n",
    "        for heap in buckets:\n",
    "            feature_results = []\n",
    "            for val, tok_id in sorted(heap, key=lambda x: -x[0]):  # Sort directly here\n",
    "                feature_results.append((float(val), token_id_to_text[tok_id]))\n",
    "            decoded.append(feature_results)  # Already sorted\n",
    "        \n",
    "        print(f\"Dead features: {dead_mask.sum()} / {n_features} \"\n",
    "              f\"({dead_mask.sum()/n_features*100:.2f}%)\")\n",
    "        \n",
    "        return decoded, dead_mask\n",
    "\n",
    "    data_iter = residual_stream_iter(dataset_iter)\n",
    "    target_tokens = 50_000_000\n",
    "    with torch.inference_mode():\n",
    "        top_buckets, dead_mask = mine_top_tokens_and_dead(\n",
    "            data_iter,\n",
    "            top_n=50,\n",
    "            target_tokens=50_000_000\n",
    "        )\n",
    "\n",
    "    # Save results - using faster formats where possible\n",
    "    # Use pickle for faster serialization\n",
    "    with open(DUMP_DIR / f\"top_tokens_{target_tokens}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(top_buckets, f)\n",
    "    \n",
    "    with open(DUMP_DIR / f\"top_tokens_{target_tokens}.json\", \"w\") as f:\n",
    "        json.dump(top_buckets, f)\n",
    "    \n",
    "    np.save(DUMP_DIR / f\"dead_features_{target_tokens}.npy\", dead_mask)\n",
    "    \n",
    "    pd.Series(dead_mask).to_csv(DUMP_DIR / f\"dead_features_{target_tokens}.csv\", index=False)\n",
    "\n",
    "# Run with profiling\n",
    "if __name__ == \"__main__\":\n",
    "    profiler = cProfile.Profile()\n",
    "    profiler.enable()\n",
    "    \n",
    "    main()\n",
    "    \n",
    "    profiler.disable()\n",
    "    \n",
    "    # Save stats to a file\n",
    "    stats = pstats.Stats(profiler)\n",
    "    stats.sort_stats('cumtime')\n",
    "    stats.dump_stats('profile_results.prof')\n",
    "    \n",
    "    print(\"\\n\\n--- Profiling Results ---\")\n",
    "    stats.sort_stats('cumtime').print_stats(20)\n",
    "    \n",
    "    print(\"\\n\\n--- Profiling Results by Function Calls ---\")\n",
    "    stats.sort_stats('calls').print_stats(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65536/65536 [37:15<00:00, 29.31it/s]  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_id</th>\n",
       "      <th>label</th>\n",
       "      <th>chain_of_thought</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>stop word</td>\n",
       "      <td>The tokens are all common English stop words, ...</td>\n",
       "      <td>[(2.8125,  the), (2.671875, ane), (2.59375,  i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Worth</td>\n",
       "      <td>The word 'worth' appears frequently, suggestin...</td>\n",
       "      <td>[(3.234375,  can), (3.140625,  Worth), (3.0312...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>punctuation</td>\n",
       "      <td>The tokens are all punctuation marks.</td>\n",
       "      <td>[(4.5, .\\n), (4.1875, \\n), (4.125, \\n), (3.828...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>uncertain</td>\n",
       "      <td>The tokens appear to be a mix of common words,...</td>\n",
       "      <td>[(3.0625, _SCHEMA), (2.859375, _SCHEMA), (2.81...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>atheism</td>\n",
       "      <td>The tokens frequently co-occur with variations...</td>\n",
       "      <td>[(2.09375,  athe), (2.03125,  Evans), (2.01562...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_id        label                                   chain_of_thought  \\\n",
       "0           0    stop word  The tokens are all common English stop words, ...   \n",
       "1           1        Worth  The word 'worth' appears frequently, suggestin...   \n",
       "2           2  punctuation              The tokens are all punctuation marks.   \n",
       "3           3    uncertain  The tokens appear to be a mix of common words,...   \n",
       "4           4      atheism  The tokens frequently co-occur with variations...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [(2.8125,  the), (2.671875, ane), (2.59375,  i...  \n",
       "1  [(3.234375,  can), (3.140625,  Worth), (3.0312...  \n",
       "2  [(4.5, .\\n), (4.1875, \\n), (4.125, \\n), (3.828...  \n",
       "3  [(3.0625, _SCHEMA), (2.859375, _SCHEMA), (2.81...  \n",
       "4  [(2.09375,  athe), (2.03125,  Evans), (2.01562...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio, pickle, pandas as pd, instructor\n",
    "from openai import AsyncOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from tqdm.asyncio import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "client = instructor.from_openai(\n",
    "    AsyncOpenAI(base_url=\"https://openrouter.ai/api/v1\", api_key=os.environ.get(\"OPENROUTER_API_KEY\"))\n",
    ")\n",
    "\n",
    "class FeatureLabel(BaseModel):\n",
    "    label: str = Field(..., description=\"≤5-word noun phrase or 'uncertain'\")\n",
    "    chain_of_thought: str = Field(..., description=\"One-sentence rationale\")\n",
    "\n",
    "\n",
    "def make_prompt(tokens: list[tuple]) -> str:\n",
    "    token_str = \", \".join(str(tok) for _, tok in tokens)\n",
    "\n",
    "    return (\n",
    "        \"You will receive several tokens that all activate *the same* hidden feature.\\n\\n\"\n",
    "        f\"Tokens: {token_str}\\n\\n\"\n",
    "        \"Return **only** the following JSON object (no additional text):\\n\"\n",
    "        \"{\\n\"\n",
    "        '  \"label\": \"<≤5-word noun phrase, or \\'uncertain\\', or \\'punctuation\\'>\",\\n'\n",
    "        '  \"chain_of_thought\": \"<one concise sentence explaining why>\"\\n'\n",
    "        \"}\\n\\n\"\n",
    "        \"• Use \\\"uncertain\\\" if no coherent feature emerges.\\n\"\n",
    "        \"• Use \\\"punctuation\\\" if the tokens are mostly punctuation or formatting marks.\\n\"\n",
    "        \"• Do not output anything except that JSON object.\"\n",
    "    )\n",
    "\n",
    "\n",
    "top_buckets = pickle.load(open(\"results/top_tokens_50m.pkl\", \"rb\"))\n",
    "sem = asyncio.Semaphore(20)\n",
    "\n",
    "async def label_one(idx, tokens):\n",
    "    prompt = make_prompt(tokens)\n",
    "    async with sem:\n",
    "        fl = await client.chat.completions.create(\n",
    "            model=\"google/gemini-2.0-flash-001\",\n",
    "            response_model=FeatureLabel,\n",
    "            temperature=0.6,\n",
    "            messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "        )\n",
    "    return {\n",
    "        \"feature_id\": idx,\n",
    "        \"label\": fl.label,\n",
    "        \"chain_of_thought\": fl.chain_of_thought,\n",
    "        \"tokens\": tokens,\n",
    "    }\n",
    "\n",
    "async def main():\n",
    "    tasks = [asyncio.create_task(label_one(i, t)) for i, t in enumerate(top_buckets)]\n",
    "    return await tqdm.gather(*tasks)\n",
    "\n",
    "rows = await main()\n",
    "\n",
    "df = pd.DataFrame(rows).sort_values(\"feature_id\")\n",
    "df.to_csv(\"results/feature_labels.csv\", index=False)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henry/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 49.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What capabilities do you have as an AI assistant?\n",
      "OLMo: As OLMo 2, I have a wide range of capabilities! Some of the key ones include understanding and generating human-like text, answering questions across many topics, and assisting with a variety of tasks such as scheduling, language translation, summarizing texts, and much more. I'm designed to continuously improve my responses based on the feedback I receive, so I'm here to learn and help you better over time. How can I assist you today?\n",
      "--------------------------------------------------\n",
      "User: Can you give me an example of how you might help with coding?\n",
      "OLMo: Certainly! While I don't write or debug code myself, I can certainly help guide you through concepts, explain coding principles, or help you understand a particular problem you might be facing with coding. For example, if you're trying to solve a problem using Python and you're unsure about the syntax, I can provide you with the correct syntax or direct you to resources where you can learn more. Here's an example of a simple Python function:\n",
      "\n",
      "```python\n",
      "def greet(person):\n",
      "    if person == \"John\":\n",
      "        return \"Hello, John!\"\n",
      "    else:\n",
      "        return \"Hello, stranger!\"\n",
      "\n",
      "# Usage\n",
      "print(greet(\"John\"))  # Output: Hello, John!\n",
      "print(greet(\"Alice\"))  # Output: Hello, stranger!\n",
      "```\n",
      "\n",
      "This function greets a person differently based on their name. If you have any questions about this code or need help with coding in general, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"allenai/OLMo-2-1124-7B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "default_system_prompt = \"You are OLMo 2, a helpful and harmless AI Assistant built by the Allen Institute for AI.\"\n",
    "\n",
    "def chat_with_olmo(user_message, system_prompt=default_system_prompt, history=None):\n",
    "    if history is None:\n",
    "        history = []\n",
    "    \n",
    "    # Create messages list as before\n",
    "    messages = []\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    for h in history:\n",
    "        messages.append(h)\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize with explicit attention mask\n",
    "    inputs = tokenizer(\n",
    "        formatted_prompt, \n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        return_attention_mask=True  # Explicitly request attention mask\n",
    "    ).to(device)\n",
    "    \n",
    "    # Generate with attention mask\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,  # Pass the attention mask\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Rest of the function remains the same\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    assistant_response = full_response.split(\"<|assistant|>\")[-1].split(\"<|endoftext|>\")[0].strip()\n",
    "    \n",
    "    history.append({\"role\": \"user\", \"content\": user_message})\n",
    "    history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "    \n",
    "    return assistant_response, history\n",
    "\n",
    "# Example usage\n",
    "conversation_history = []\n",
    "\n",
    "# First exchange\n",
    "user_input = \"What capabilities do you have as an AI assistant?\"\n",
    "response, conversation_history = chat_with_olmo(user_input, history=conversation_history)\n",
    "print(f\"User: {user_input}\")\n",
    "print(f\"OLMo: {response}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Follow-up question (demonstrating conversation history)\n",
    "user_input = \"Can you give me an example of how you might help with coding?\"\n",
    "response, conversation_history = chat_with_olmo(user_input, history=conversation_history)\n",
    "print(f\"User: {user_input}\")\n",
    "print(f\"OLMo: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Olmo2ForCausalLM\n",
    "\n",
    "class SteeredOlmo(Olmo2ForCausalLM):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
