{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored When destroying _lsprof profiler:\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1253868/3328758826.py\", line 210, in <module>\n",
      "RuntimeError: Cannot install a profile function while another profile function is being installed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SAE with 65536 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.13s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.57it/s]\n",
      "Some weights of the model checkpoint at allenai/OLMo-2-1124-7B-Instruct were not used when initializing Olmo2ForCausalLM: ['model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.16.post_feedforward_layernorm.weight', 'model.layers.16.self_attn.k_norm.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_norm.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.17.post_feedforward_layernorm.weight', 'model.layers.17.self_attn.k_norm.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_norm.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.18.post_feedforward_layernorm.weight', 'model.layers.18.self_attn.k_norm.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_norm.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.19.post_feedforward_layernorm.weight', 'model.layers.19.self_attn.k_norm.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_norm.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.20.post_feedforward_layernorm.weight', 'model.layers.20.self_attn.k_norm.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_norm.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.21.post_feedforward_layernorm.weight', 'model.layers.21.self_attn.k_norm.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_norm.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.22.post_feedforward_layernorm.weight', 'model.layers.22.self_attn.k_norm.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_norm.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.23.post_feedforward_layernorm.weight', 'model.layers.23.self_attn.k_norm.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_norm.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.24.post_feedforward_layernorm.weight', 'model.layers.24.self_attn.k_norm.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_norm.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.25.post_feedforward_layernorm.weight', 'model.layers.25.self_attn.k_norm.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_norm.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.26.post_feedforward_layernorm.weight', 'model.layers.26.self_attn.k_norm.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_norm.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.27.post_feedforward_layernorm.weight', 'model.layers.27.self_attn.k_norm.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_norm.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.28.post_feedforward_layernorm.weight', 'model.layers.28.self_attn.k_norm.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_norm.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.29.post_feedforward_layernorm.weight', 'model.layers.29.self_attn.k_norm.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_norm.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.30.post_feedforward_layernorm.weight', 'model.layers.30.self_attn.k_norm.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_norm.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.31.post_feedforward_layernorm.weight', 'model.layers.31.self_attn.k_norm.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_norm.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight']\n",
      "- This IS expected if you are initializing Olmo2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Olmo2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 49999104 tokens (2243.5 tok/s)\n",
      "Processed 50000128 tokens in 22286.87s\n",
      "Dead features: 2336 / 65536 (3.56%)\n",
      "\n",
      "\n",
      "--- Profiling Results ---\n",
      "         13146823416 function calls (13117207214 primitive calls) in 22375.153 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 15830 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    4.105    4.105 22376.231 22376.231 /tmp/ipykernel_1253868/3328758826.py:66(main)\n",
      "        1 12310.012 12310.012 22294.526 22294.526 /tmp/ipykernel_1253868/3328758826.py:101(mine_top_tokens_and_dead)\n",
      "   585940 8301.520    0.014 8301.520    0.014 {method 'cpu' of 'torch._C.TensorBase' objects}\n",
      "   195314   13.252    0.000  870.551    0.004 /tmp/ipykernel_1253868/3328758826.py:89(residual_stream_iter)\n",
      "12803479658/12803373515  705.880    0.000  705.909    0.000 {built-in method builtins.len}\n",
      "585939/390626    2.018    0.000  338.463    0.001 /home/henry/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747(_wrapped_call_impl)\n",
      "585939/390626  110.117    0.000  337.127    0.001 /home/henry/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1755(_call_impl)\n",
      "    74063    0.533    0.000  334.909    0.005 /home/henry/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:2372(__iter__)\n",
      "    74062    0.883    0.000  334.376    0.005 /home/henry/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:2747(_getitem)\n",
      "    74062    0.329    0.000  320.792    0.004 /home/henry/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py:616(format_table)\n",
      "    74062    0.072    0.000  320.063    0.004 /home/henry/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py:404(__call__)\n",
      "    74062    0.286    0.000  319.991    0.004 /home/henry/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py:451(format_row)\n",
      "    74062  315.853    0.004  316.376    0.004 /home/henry/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py:142(extract_row)\n",
      "   195313    3.294    0.000  307.955    0.002 /home/henry/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:619(_fn)\n",
      "    74062    1.068    0.000  162.809    0.002 /home/henry/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2790(__call__)\n",
      "   195313   17.383    0.000  162.361    0.001 /home/henry/.local/lib/python3.10/site-packages/transformers/utils/generic.py:953(wrapper)\n",
      "    74062    0.465    0.000  161.626    0.002 /home/henry/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2882(_call_one)\n",
      "    74062    0.602    0.000  161.068    0.002 /home/henry/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3000(encode_plus)\n",
      "    74062    0.353    0.000  159.964    0.002 /home/henry/.local/lib/python3.10/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py:118(_encode_plus)\n",
      "    74062    0.852    0.000  159.589    0.002 /home/henry/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:591(_encode_plus)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Profiling Results by Function Calls ---\n",
      "         13146823416 function calls (13117207214 primitive calls) in 22375.153 seconds\n",
      "\n",
      "   Ordered by: call count\n",
      "   List reduced from 15830 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "12803479658/12803373515  705.880    0.000  705.909    0.000 {built-in method builtins.len}\n",
      " 80083771    9.063    0.000    9.064    0.000 {function ClassInstantier.__getitem__ at 0x7099f415e560}\n",
      "42157246/42032662    4.698    0.000    5.703    0.000 {built-in method builtins.isinstance}\n",
      "39583750/13238278   16.648    0.000   21.879    0.000 /usr/lib/python3.10/json/encoder.py:277(_iterencode_list)\n",
      " 26521099   46.608    0.000   46.608    0.000 {built-in method _heapq.heapreplace}\n",
      " 13238518    2.888    0.000   24.767    0.000 /usr/lib/python3.10/json/encoder.py:413(_iterencode)\n",
      " 13238276    1.359    0.000    1.359    0.000 {method 'write' of '_io.TextIOWrapper' objects}\n",
      "  9661468    0.874    0.000    0.874    0.000 {method 'append' of 'list' objects}\n",
      "4192980/3798878    0.738    0.000    0.892    0.000 {built-in method builtins.hash}\n",
      "  4032738    0.472    0.000    0.476    0.000 {method 'add' of 'set' objects}\n",
      "  3664720    0.388    0.000    0.388    0.000 {built-in method builtins.id}\n",
      "  3277103    1.244    0.000    1.244    0.000 {built-in method _heapq.heappush}\n",
      "  3276859    0.383    0.000    0.383    0.000 {built-in method _json.encode_basestring_ascii}\n",
      "  3276804    2.297    0.000    2.297    0.000 /usr/lib/python3.10/json/encoder.py:223(floatstr)\n",
      "  3276800    0.668    0.000    0.668    0.000 /tmp/ipykernel_1253868/3328758826.py:175(<lambda>)\n",
      "  2734370    0.399    0.000    0.399    0.000 /home/henry/.local/lib/python3.10/site-packages/torch/_inductor/cudagraph_trees.py:2322(current_node)\n",
      "2077892/2040728    0.538    0.000    0.657    0.000 {built-in method builtins.getattr}\n",
      "  1999674    0.335    0.000    0.335    0.000 /home/henry/.local/lib/python3.10/site-packages/datasets/utils/py_utils.py:336(<genexpr>)\n",
      "  1629797    0.333    0.000    0.397    0.000 {method 'get' of 'dict' objects}\n",
      "  1562660    0.630    0.000    0.630    0.000 {built-in method torch._C._dynamo.eval_frame.set_eval_frame}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import heapq\n",
    "import pickle\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cProfile\n",
    "import pstats\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from psutil import cpu_count\n",
    "\n",
    "SAE_PATH      = Path('out/sae_65k_lambda26_ramp30/sae_final.pt')\n",
    "TOP_N         = 50\n",
    "DUMP_DIR      = Path('feature_dumps')\n",
    "DUMP_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n",
    "# ---- Load SAE (replace with your own class/loader) ----\n",
    "from model import SAE\n",
    "state_dict, config = torch.load(SAE_PATH, map_location=device).values()\n",
    "sae = SAE(config['input_size'],config['hidden_size']).to(device).to(torch.bfloat16)\n",
    "# Fix for \"_orig_mod\" prefix in state dict keys\n",
    "fixed_state_dict = {k.replace('_orig_mod.', ''): v for k, v in state_dict.items()}\n",
    "sae.load_state_dict(fixed_state_dict)\n",
    "\n",
    "n_features = sae.encode.out_features if hasattr(sae.encode,'out_features') else sae.n_features\n",
    "print(f'Loaded SAE with {n_features} features')\n",
    "\n",
    "def count_dead_features(sample_iter, sample_tokens=10_000_000):\n",
    "    \"\"\"Returns a boolean tensor of shape (n_features,) where True == dead.\"\"\"\n",
    "    fired = torch.zeros(n_features, dtype=torch.bool, device=device)\n",
    "    seen  = 0\n",
    "    for toks in tqdm(sample_iter, total=sample_tokens//len(next(iter(sample_iter)))):\n",
    "        toks = toks.to(device)\n",
    "        acts = sae.encode(toks) > 0  # bool mask of activations\n",
    "        fired |= acts.any(dim=0)\n",
    "        seen  += toks.size(0)\n",
    "        if seen >= sample_tokens:\n",
    "            break\n",
    "    dead_mask = ~fired.cpu()\n",
    "    print(f\"Dead features: {dead_mask.sum().item()} / {n_features} ({dead_mask.float().mean()*100:.2f}%)\")\n",
    "    return dead_mask\n",
    "\n",
    "# GPU optimizations\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# Constants\n",
    "MODEL_NAME = \"allenai/OLMo-2-1124-7B-Instruct\"\n",
    "BATCH_SIZE = 256\n",
    "LAYER_OFFSET = -1\n",
    "TOP_N = 50\n",
    "device = \"cuda\"\n",
    "DUMP_DIR = Path(\"./results\")\n",
    "os.makedirs(DUMP_DIR, exist_ok=True)\n",
    "\n",
    "def main():\n",
    "    # Load tokenizer + *half* model config (bf16, compiled)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    cfg = AutoModelForCausalLM.from_pretrained(MODEL_NAME).config\n",
    "    cfg.num_hidden_layers //= 2  # half-model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        config=cfg,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        attn_implementation=\"sdpa\",\n",
    "        device_map=device\n",
    "    ).eval()\n",
    "    model = torch.compile(model, mode=\"reduce-overhead\")\n",
    "\n",
    "    # Load dataset\n",
    "    dataset_iter = load_dataset(\n",
    "        \"HuggingFaceFW/fineweb\",\n",
    "        name=\"sample-10BT\",\n",
    "        split=\"train\",\n",
    "        streaming=False,\n",
    "        num_proc=cpu_count(),\n",
    "    ).shuffle()\n",
    "\n",
    "    def residual_stream_iter(text_iter, batch=BATCH_SIZE):\n",
    "        buf = []\n",
    "        for record in text_iter:\n",
    "            buf.extend(tokenizer(record[\"text\"]).input_ids)\n",
    "            while len(buf) >= batch:\n",
    "                toks = torch.tensor(buf[:batch]).to(device)\n",
    "                buf = buf[batch:]\n",
    "                with torch.inference_mode():\n",
    "                    outs = model(toks.unsqueeze(0), output_hidden_states=True)\n",
    "                    resid = outs.hidden_states[LAYER_OFFSET].squeeze(0)  # (T, d)\n",
    "                yield toks, resid  # feed straight to SAE\n",
    "\n",
    "    def mine_top_tokens_and_dead(data_iter,\n",
    "                              top_n=TOP_N,\n",
    "                              target_tokens=100_000):\n",
    "        \"\"\"\n",
    "        • Keeps the TOP-N strongest (activation, token) pairs per feature.\n",
    "        • Tracks which features ever fire to flag the 'dead' ones.\n",
    "        • Stops after `target_tokens` have been processed.\n",
    "        \"\"\"\n",
    "        n_features = sae.encode.weight.shape[0]\n",
    "        \n",
    "        # Pre-allocate all buckets with empty heaps\n",
    "        buckets = [[] for _ in range(n_features)]\n",
    "        fired = torch.zeros(n_features, dtype=torch.bool, device=device)\n",
    "        seen_toks = 0\n",
    "        \n",
    "        # Process batches with lighter progress indicator \n",
    "        start_time = time.time()\n",
    "        batch_count = 0\n",
    "        \n",
    "        for toks, resid in data_iter:\n",
    "            batch_count += 1\n",
    "            if batch_count % 10 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                tokens_per_sec = seen_toks / elapsed if elapsed > 0 else 0\n",
    "                print(f\"\\rProcessed {seen_toks} tokens ({tokens_per_sec:.1f} tok/s)\", end=\"\")\n",
    "                \n",
    "            with torch.inference_mode():\n",
    "                # Compute activations\n",
    "                acts = sae.encode(resid)\n",
    "                fired |= (acts > 0).any(dim=0)\n",
    "                \n",
    "                # Get top values and indices\n",
    "                values, idx = acts.topk(1, dim=0)\n",
    "                \n",
    "                # Transfer to CPU in one batch  \n",
    "                values_cpu = values[0].to(torch.float32).detach().cpu().numpy()\n",
    "                indices_cpu = idx[0].detach().cpu().numpy()\n",
    "                token_ids = toks[indices_cpu].cpu().numpy()\n",
    "                \n",
    "                # Process features in chunks for better performance\n",
    "                for f in range(n_features):\n",
    "                    val, tok_id = float(values_cpu[f]), int(token_ids[f])\n",
    "                    heap = buckets[f]\n",
    "                    if len(heap) < top_n:\n",
    "                        heapq.heappush(heap, (val, tok_id))\n",
    "                    elif val > heap[0][0]:\n",
    "                        heapq.heapreplace(heap, (val, tok_id))\n",
    "            \n",
    "            seen_toks += toks.numel()\n",
    "            if seen_toks >= target_tokens:\n",
    "                break\n",
    "        \n",
    "        print(f\"\\nProcessed {seen_toks} tokens in {time.time() - start_time:.2f}s\")\n",
    "        \n",
    "        # Post-process\n",
    "        dead_mask = ~fired.cpu().numpy()  # Convert directly to numpy\n",
    "        \n",
    "        # Get unique token IDs for batch decoding\n",
    "        unique_token_ids = set()\n",
    "        for heap in buckets:\n",
    "            for _, tok_id in heap:\n",
    "                unique_token_ids.add(tok_id)\n",
    "        \n",
    "        # Convert set to list for batch decoding\n",
    "        unique_token_list = list(unique_token_ids)\n",
    "        decoded_tokens = tokenizer.batch_decode([[t] for t in unique_token_list])\n",
    "        \n",
    "        # Create mapping of token ID to decoded text\n",
    "        token_id_to_text = {unique_token_list[i]: decoded_tokens[i] for i in range(len(unique_token_list))}\n",
    "        \n",
    "        # Create the final result with native Python types\n",
    "        decoded = []\n",
    "        for heap in buckets:\n",
    "            feature_results = []\n",
    "            for val, tok_id in sorted(heap, key=lambda x: -x[0]):  # Sort directly here\n",
    "                feature_results.append((float(val), token_id_to_text[tok_id]))\n",
    "            decoded.append(feature_results)  # Already sorted\n",
    "        \n",
    "        print(f\"Dead features: {dead_mask.sum()} / {n_features} \"\n",
    "              f\"({dead_mask.sum()/n_features*100:.2f}%)\")\n",
    "        \n",
    "        return decoded, dead_mask\n",
    "\n",
    "    data_iter = residual_stream_iter(dataset_iter)\n",
    "    target_tokens = 50_000_000\n",
    "    with torch.inference_mode():\n",
    "        top_buckets, dead_mask = mine_top_tokens_and_dead(\n",
    "            data_iter,\n",
    "            top_n=50,\n",
    "            target_tokens=50_000_000\n",
    "        )\n",
    "\n",
    "    # Save results - using faster formats where possible\n",
    "    # Use pickle for faster serialization\n",
    "    with open(DUMP_DIR / f\"top_tokens_{target_tokens}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(top_buckets, f)\n",
    "    \n",
    "    with open(DUMP_DIR / f\"top_tokens_{target_tokens}.json\", \"w\") as f:\n",
    "        json.dump(top_buckets, f)\n",
    "    \n",
    "    np.save(DUMP_DIR / f\"dead_features_{target_tokens}.npy\", dead_mask)\n",
    "    \n",
    "    pd.Series(dead_mask).to_csv(DUMP_DIR / f\"dead_features_{target_tokens}.csv\", index=False)\n",
    "\n",
    "# Run with profiling\n",
    "if __name__ == \"__main__\":\n",
    "    profiler = cProfile.Profile()\n",
    "    profiler.enable()\n",
    "    \n",
    "    main()\n",
    "    \n",
    "    profiler.disable()\n",
    "    \n",
    "    # Save stats to a file\n",
    "    stats = pstats.Stats(profiler)\n",
    "    stats.sort_stats('cumtime')\n",
    "    stats.dump_stats('profile_results.prof')\n",
    "    \n",
    "    print(\"\\n\\n--- Profiling Results ---\")\n",
    "    stats.sort_stats('cumtime').print_stats(20)\n",
    "    \n",
    "    print(\"\\n\\n--- Profiling Results by Function Calls ---\")\n",
    "    stats.sort_stats('calls').print_stats(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 4296/65536 [02:31<36:05, 28.28it/s]  "
     ]
    }
   ],
   "source": [
    "import asyncio, pickle, pandas as pd, instructor\n",
    "from openai import AsyncOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from tqdm.asyncio import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "client = instructor.from_openai(\n",
    "    AsyncOpenAI(base_url=\"https://openrouter.ai/api/v1\", api_key=os.environ.get(\"OPENROUTER_API_KEY\"))\n",
    ")\n",
    "\n",
    "class FeatureLabel(BaseModel):\n",
    "    label: str = Field(..., description=\"≤5-word noun phrase or 'uncertain'\")\n",
    "    chain_of_thought: str = Field(..., description=\"One-sentence rationale\")\n",
    "\n",
    "\n",
    "def make_prompt(tokens: list[tuple]) -> str:\n",
    "    token_str = \", \".join(str(tok) for _, tok in tokens)\n",
    "\n",
    "    return (\n",
    "        \"You will receive several tokens that all activate *the same* hidden feature.\\n\\n\"\n",
    "        f\"Tokens: {token_str}\\n\\n\"\n",
    "        \"Return **only** the following JSON object (no additional text):\\n\"\n",
    "        \"{\\n\"\n",
    "        '  \"label\": \"<≤5-word noun phrase, or \\'uncertain\\', or \\'punctuation\\'>\",\\n'\n",
    "        '  \"chain_of_thought\": \"<one concise sentence explaining why>\"\\n'\n",
    "        \"}\\n\\n\"\n",
    "        \"• Use \\\"uncertain\\\" if no coherent feature emerges.\\n\"\n",
    "        \"• Use \\\"punctuation\\\" if the tokens are mostly punctuation or formatting marks.\\n\"\n",
    "        \"• Do not output anything except that JSON object.\"\n",
    "    )\n",
    "\n",
    "\n",
    "top_buckets = pickle.load(open(\"results/top_tokens_50m.pkl\", \"rb\"))\n",
    "sem = asyncio.Semaphore(20)\n",
    "\n",
    "async def label_one(idx, tokens):\n",
    "    prompt = make_prompt(tokens)\n",
    "    async with sem:\n",
    "        fl = await client.chat.completions.create(\n",
    "            model=\"google/gemini-2.0-flash-001\",\n",
    "            response_model=FeatureLabel,\n",
    "            temperature=0.6,\n",
    "            messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "        )\n",
    "    return {\n",
    "        \"feature_id\": idx,\n",
    "        \"label\": fl.label,\n",
    "        \"chain_of_thought\": fl.chain_of_thought,\n",
    "        \"tokens\": tokens,\n",
    "    }\n",
    "\n",
    "async def main():\n",
    "    tasks = [asyncio.create_task(label_one(i, t)) for i, t in enumerate(top_buckets)]\n",
    "    return await tqdm.gather(*tasks)\n",
    "\n",
    "rows = await main()\n",
    "\n",
    "df = pd.DataFrame(rows).sort_values(\"feature_id\")\n",
    "df.to_csv(\"results/feature_labels.csv\", index=False)\n",
    "df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
