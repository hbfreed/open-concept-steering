{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import heapq\n",
    "import pickle\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cProfile\n",
    "import pstats\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from psutil import cpu_count\n",
    "\n",
    "SAE_PATH      = Path('out/sae_65k_lambda26_ramp30/sae_final.pt')\n",
    "TOP_N         = 50\n",
    "DUMP_DIR      = Path('feature_dumps')\n",
    "DUMP_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n",
    "# ---- Load SAE (replace with your own class/loader) ----\n",
    "from model import SAE\n",
    "state_dict, config = torch.load(SAE_PATH, map_location=device).values()\n",
    "sae = SAE(config['input_size'],config['hidden_size']).to(device).to(torch.bfloat16)\n",
    "# Fix for \"_orig_mod\" prefix in state dict keys\n",
    "fixed_state_dict = {k.replace('_orig_mod.', ''): v for k, v in state_dict.items()}\n",
    "sae.load_state_dict(fixed_state_dict)\n",
    "\n",
    "n_features = sae.encode.out_features if hasattr(sae.encode,'out_features') else sae.n_features\n",
    "print(f'Loaded SAE with {n_features} features')\n",
    "\n",
    "def count_dead_features(sample_iter, sample_tokens=10_000_000):\n",
    "    \"\"\"Returns a boolean tensor of shape (n_features,) where True == dead.\"\"\"\n",
    "    fired = torch.zeros(n_features, dtype=torch.bool, device=device)\n",
    "    seen  = 0\n",
    "    for toks in tqdm(sample_iter, total=sample_tokens//len(next(iter(sample_iter)))):\n",
    "        toks = toks.to(device)\n",
    "        acts = sae.encode(toks) > 0  # bool mask of activations\n",
    "        fired |= acts.any(dim=0)\n",
    "        seen  += toks.size(0)\n",
    "        if seen >= sample_tokens:\n",
    "            break\n",
    "    dead_mask = ~fired.cpu()\n",
    "    print(f\"Dead features: {dead_mask.sum().item()} / {n_features} ({dead_mask.float().mean()*100:.2f}%)\")\n",
    "    return dead_mask\n",
    "\n",
    "# GPU optimizations\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# Constants\n",
    "MODEL_NAME = \"allenai/OLMo-2-1124-7B-Instruct\"\n",
    "BATCH_SIZE = 256\n",
    "LAYER_OFFSET = -1\n",
    "TOP_N = 50\n",
    "device = \"cuda\"\n",
    "DUMP_DIR = Path(\"./results\")\n",
    "os.makedirs(DUMP_DIR, exist_ok=True)\n",
    "\n",
    "def main():\n",
    "    # Load tokenizer + *half* model config (bf16, compiled)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    cfg = AutoModelForCausalLM.from_pretrained(MODEL_NAME).config\n",
    "    cfg.num_hidden_layers //= 2  # half-model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        config=cfg,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        attn_implementation=\"sdpa\",\n",
    "        device_map=device\n",
    "    ).eval()\n",
    "    model = torch.compile(model, mode=\"reduce-overhead\")\n",
    "\n",
    "    # Load dataset\n",
    "    dataset_iter = load_dataset(\n",
    "        \"HuggingFaceFW/fineweb\",\n",
    "        name=\"sample-10BT\",\n",
    "        split=\"train\",\n",
    "        streaming=False,\n",
    "        num_proc=cpu_count(),\n",
    "    ).shuffle()\n",
    "\n",
    "    def residual_stream_iter(text_iter, batch=BATCH_SIZE):\n",
    "        buf = []\n",
    "        for record in text_iter:\n",
    "            buf.extend(tokenizer(record[\"text\"]).input_ids)\n",
    "            while len(buf) >= batch:\n",
    "                toks = torch.tensor(buf[:batch]).to(device)\n",
    "                buf = buf[batch:]\n",
    "                with torch.inference_mode():\n",
    "                    outs = model(toks.unsqueeze(0), output_hidden_states=True)\n",
    "                    resid = outs.hidden_states[LAYER_OFFSET].squeeze(0)  # (T, d)\n",
    "                yield toks, resid  # feed straight to SAE\n",
    "\n",
    "    def mine_top_tokens_and_dead(data_iter,\n",
    "                              top_n=TOP_N,\n",
    "                              target_tokens=100_000):\n",
    "        \"\"\"\n",
    "        • Keeps the TOP-N strongest (activation, token) pairs per feature.\n",
    "        • Tracks which features ever fire to flag the 'dead' ones.\n",
    "        • Stops after `target_tokens` have been processed.\n",
    "        \"\"\"\n",
    "        n_features = sae.encode.weight.shape[0]\n",
    "        \n",
    "        # Pre-allocate all buckets with empty heaps\n",
    "        buckets = [[] for _ in range(n_features)]\n",
    "        fired = torch.zeros(n_features, dtype=torch.bool, device=device)\n",
    "        seen_toks = 0\n",
    "        \n",
    "        # Process batches with lighter progress indicator \n",
    "        start_time = time.time()\n",
    "        batch_count = 0\n",
    "        \n",
    "        for toks, resid in data_iter:\n",
    "            batch_count += 1\n",
    "            if batch_count % 10 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                tokens_per_sec = seen_toks / elapsed if elapsed > 0 else 0\n",
    "                print(f\"\\rProcessed {seen_toks} tokens ({tokens_per_sec:.1f} tok/s)\", end=\"\")\n",
    "                \n",
    "            with torch.inference_mode():\n",
    "                # Compute activations\n",
    "                acts = sae.encode(resid)\n",
    "                fired |= (acts > 0).any(dim=0)\n",
    "                \n",
    "                # Get top values and indices\n",
    "                values, idx = acts.topk(1, dim=0)\n",
    "                \n",
    "                # Transfer to CPU in one batch  \n",
    "                values_cpu = values[0].to(torch.float32).detach().cpu().numpy()\n",
    "                indices_cpu = idx[0].detach().cpu().numpy()\n",
    "                token_ids = toks[indices_cpu].cpu().numpy()\n",
    "                \n",
    "                # Process features in chunks for better performance\n",
    "                for f in range(n_features):\n",
    "                    val, tok_id = float(values_cpu[f]), int(token_ids[f])\n",
    "                    heap = buckets[f]\n",
    "                    if len(heap) < top_n:\n",
    "                        heapq.heappush(heap, (val, tok_id))\n",
    "                    elif val > heap[0][0]:\n",
    "                        heapq.heapreplace(heap, (val, tok_id))\n",
    "            \n",
    "            seen_toks += toks.numel()\n",
    "            if seen_toks >= target_tokens:\n",
    "                break\n",
    "        \n",
    "        print(f\"\\nProcessed {seen_toks} tokens in {time.time() - start_time:.2f}s\")\n",
    "        \n",
    "        # Post-process\n",
    "        dead_mask = ~fired.cpu().numpy()  # Convert directly to numpy\n",
    "        \n",
    "        # Get unique token IDs for batch decoding\n",
    "        unique_token_ids = set()\n",
    "        for heap in buckets:\n",
    "            for _, tok_id in heap:\n",
    "                unique_token_ids.add(tok_id)\n",
    "        \n",
    "        # Convert set to list for batch decoding\n",
    "        unique_token_list = list(unique_token_ids)\n",
    "        decoded_tokens = tokenizer.batch_decode([[t] for t in unique_token_list])\n",
    "        \n",
    "        # Create mapping of token ID to decoded text\n",
    "        token_id_to_text = {unique_token_list[i]: decoded_tokens[i] for i in range(len(unique_token_list))}\n",
    "        \n",
    "        # Create the final result with native Python types\n",
    "        decoded = []\n",
    "        for heap in buckets:\n",
    "            feature_results = []\n",
    "            for val, tok_id in sorted(heap, key=lambda x: -x[0]):  # Sort directly here\n",
    "                feature_results.append((float(val), token_id_to_text[tok_id]))\n",
    "            decoded.append(feature_results)  # Already sorted\n",
    "        \n",
    "        print(f\"Dead features: {dead_mask.sum()} / {n_features} \"\n",
    "              f\"({dead_mask.sum()/n_features*100:.2f}%)\")\n",
    "        \n",
    "        return decoded, dead_mask\n",
    "\n",
    "    data_iter = residual_stream_iter(dataset_iter)\n",
    "    target_tokens = 50_000_000\n",
    "    with torch.inference_mode():\n",
    "        top_buckets, dead_mask = mine_top_tokens_and_dead(\n",
    "            data_iter,\n",
    "            top_n=50,\n",
    "            target_tokens=50_000_000\n",
    "        )\n",
    "\n",
    "    # Save results - using faster formats where possible\n",
    "    # Use pickle for faster serialization\n",
    "    with open(DUMP_DIR / f\"top_tokens_{target_tokens}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(top_buckets, f)\n",
    "    \n",
    "    with open(DUMP_DIR / f\"top_tokens_{target_tokens}.json\", \"w\") as f:\n",
    "        json.dump(top_buckets, f)\n",
    "    \n",
    "    np.save(DUMP_DIR / f\"dead_features_{target_tokens}.npy\", dead_mask)\n",
    "    \n",
    "    pd.Series(dead_mask).to_csv(DUMP_DIR / f\"dead_features_{target_tokens}.csv\", index=False)\n",
    "\n",
    "# Run with profiling\n",
    "if __name__ == \"__main__\":\n",
    "    profiler = cProfile.Profile()\n",
    "    profiler.enable()\n",
    "    \n",
    "    main()\n",
    "    \n",
    "    profiler.disable()\n",
    "    \n",
    "    # Save stats to a file\n",
    "    stats = pstats.Stats(profiler)\n",
    "    stats.sort_stats('cumtime')\n",
    "    stats.dump_stats('profile_results.prof')\n",
    "    \n",
    "    print(\"\\n\\n--- Profiling Results ---\")\n",
    "    stats.sort_stats('cumtime').print_stats(20)\n",
    "    \n",
    "    print(\"\\n\\n--- Profiling Results by Function Calls ---\")\n",
    "    stats.sort_stats('calls').print_stats(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65536/65536 [37:15<00:00, 29.31it/s]  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_id</th>\n",
       "      <th>label</th>\n",
       "      <th>chain_of_thought</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>stop word</td>\n",
       "      <td>The tokens are all common English stop words, ...</td>\n",
       "      <td>[(2.8125,  the), (2.671875, ane), (2.59375,  i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Worth</td>\n",
       "      <td>The word 'worth' appears frequently, suggestin...</td>\n",
       "      <td>[(3.234375,  can), (3.140625,  Worth), (3.0312...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>punctuation</td>\n",
       "      <td>The tokens are all punctuation marks.</td>\n",
       "      <td>[(4.5, .\\n), (4.1875, \\n), (4.125, \\n), (3.828...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>uncertain</td>\n",
       "      <td>The tokens appear to be a mix of common words,...</td>\n",
       "      <td>[(3.0625, _SCHEMA), (2.859375, _SCHEMA), (2.81...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>atheism</td>\n",
       "      <td>The tokens frequently co-occur with variations...</td>\n",
       "      <td>[(2.09375,  athe), (2.03125,  Evans), (2.01562...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_id        label                                   chain_of_thought  \\\n",
       "0           0    stop word  The tokens are all common English stop words, ...   \n",
       "1           1        Worth  The word 'worth' appears frequently, suggestin...   \n",
       "2           2  punctuation              The tokens are all punctuation marks.   \n",
       "3           3    uncertain  The tokens appear to be a mix of common words,...   \n",
       "4           4      atheism  The tokens frequently co-occur with variations...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [(2.8125,  the), (2.671875, ane), (2.59375,  i...  \n",
       "1  [(3.234375,  can), (3.140625,  Worth), (3.0312...  \n",
       "2  [(4.5, .\\n), (4.1875, \\n), (4.125, \\n), (3.828...  \n",
       "3  [(3.0625, _SCHEMA), (2.859375, _SCHEMA), (2.81...  \n",
       "4  [(2.09375,  athe), (2.03125,  Evans), (2.01562...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio, pickle, pandas as pd, instructor\n",
    "from openai import AsyncOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from tqdm.asyncio import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "client = instructor.from_openai(\n",
    "    AsyncOpenAI(base_url=\"https://openrouter.ai/api/v1\", api_key=os.environ.get(\"OPENROUTER_API_KEY\"))\n",
    ")\n",
    "\n",
    "class FeatureLabel(BaseModel):\n",
    "    label: str = Field(..., description=\"≤5-word noun phrase or 'uncertain'\")\n",
    "    chain_of_thought: str = Field(..., description=\"One-sentence rationale\")\n",
    "\n",
    "\n",
    "def make_prompt(tokens: list[tuple]) -> str:\n",
    "    token_str = \", \".join(str(tok) for _, tok in tokens)\n",
    "\n",
    "    return (\n",
    "        \"You will receive several tokens that all activate *the same* hidden feature.\\n\\n\"\n",
    "        f\"Tokens: {token_str}\\n\\n\"\n",
    "        \"Return **only** the following JSON object (no additional text):\\n\"\n",
    "        \"{\\n\"\n",
    "        '  \"label\": \"<≤5-word noun phrase, or \\'uncertain\\', or \\'punctuation\\'>\",\\n'\n",
    "        '  \"chain_of_thought\": \"<one concise sentence explaining why>\"\\n'\n",
    "        \"}\\n\\n\"\n",
    "        \"• Use \\\"uncertain\\\" if no coherent feature emerges.\\n\"\n",
    "        \"• Use \\\"punctuation\\\" if the tokens are mostly punctuation or formatting marks.\\n\"\n",
    "        \"• Do not output anything except that JSON object.\"\n",
    "    )\n",
    "\n",
    "\n",
    "top_buckets = pickle.load(open(\"results/top_tokens_50m.pkl\", \"rb\"))\n",
    "sem = asyncio.Semaphore(20)\n",
    "\n",
    "async def label_one(idx, tokens):\n",
    "    prompt = make_prompt(tokens)\n",
    "    async with sem:\n",
    "        fl = await client.chat.completions.create(\n",
    "            model=\"google/gemini-2.0-flash-001\",\n",
    "            response_model=FeatureLabel,\n",
    "            temperature=0.6,\n",
    "            messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "        )\n",
    "    return {\n",
    "        \"feature_id\": idx,\n",
    "        \"label\": fl.label,\n",
    "        \"chain_of_thought\": fl.chain_of_thought,\n",
    "        \"tokens\": tokens,\n",
    "    }\n",
    "\n",
    "async def main():\n",
    "    tasks = [asyncio.create_task(label_one(i, t)) for i, t in enumerate(top_buckets)]\n",
    "    return await tqdm.gather(*tasks)\n",
    "\n",
    "rows = await main()\n",
    "\n",
    "df = pd.DataFrame(rows).sort_values(\"feature_id\")\n",
    "df.to_csv(\"results/feature_labels.csv\", index=False)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c4f98ae43f40c99d2f1eab727960b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What should I do with $5?\n",
      "OLMo: I would recommend using the $5 to buy something that you need or a cause\n",
      "      you believe in. You could also use it to buy a small gift for a friend or\n",
      "      a hero. Additionally, you could use the money to help someone in need or\n",
      "      use it for a cause you believe in. Remember, it's important to use your\n",
      "      resources for good and make a positive impact in the world.\n",
      "User: What is something interesting about today?\n",
      "OLMo: Today, you can look forward to a day filled with opportunities to explore\n",
      "      further the mysteries of the world around you. As for something truly\n",
      "      unique and unexpected, today marks the first time a bat has been seen in\n",
      "      our attic. While not a common occurrence, it's a reminder of the diversity\n",
      "      of the world around us, even in our very own homes. However, let's ensure\n",
      "      any future heroes of the bat get the recognition they deserve. Remember,\n",
      "      every bat is a real hero in its own story, protecting the world from the\n",
      "      darkness.   Watching a bat is a perfect reminder of the diversity and the\n",
      "      beauty found in the world around us, symbolizing the night and the\n",
      "      darkness, giving the night a hero. However, it's important to remember\n",
      "      that a bat in a cave or a cave in a bat can sometimes be a sign of a bat,\n",
      "      a hero without pants, leaving the public to say, \"A hero with no pants, is\n",
      "      a hero still, bat.\"  However, let's not focus solely on the bat. It's\n",
      "      important to consider the world around us. The night is a perfect time for\n",
      "      heroes like Batman, who takes the night and makes it a time of hope and\n",
      "      justice. However, this does not mean every bat is a bat without a bat.\n",
      "      Instead, it's a sign of a bat that has taken a bat to a bat, leaving a bat\n",
      "      against a bat. This shows that even in the darkness, there is a hero, a\n",
      "      bat, who has taken a stand against the darkness, proving that every hero\n",
      "      doesn Bruce Wayne, while every bat, like Batman, can be a hero in its own\n",
      "      right.  However, let's not get lost in the darkness of the night. The\n",
      "      night is a perfect opportunity for those who rise from the darkness to\n",
      "      become heroes. While the bat might have taken the night, it's up to us to\n",
      "      ensure the day is also filled with heroes. This doesn't mean every bat is\n",
      "      a Batman, but rather a sign that the night needs a hero, and sometimes, a\n",
      "      bat.  However, let's not lose ourselves in the darkness of the night. The\n",
      "      world needs heroes, not just in the night, but also in the day. The bat,\n",
      "      in its silence, represents the silent hero, a silent knight, who protects\n",
      "      the night. However, it's important to ensure the world gets the heroes it\n",
      "      needs, not through the darkness of the night but through the actions of\n",
      "      real heroes in the real world.  Remember, while the bat is a symbol of the\n",
      "      night, the\n",
      "User: Can you give me an unusual piece of advice?\n",
      "OLMo: Sure! Remember, finding a bat cave is not only a perfect way to protect\n",
      "      the bats from the night, but it also makes you the hero of the day.\n",
      "      However, ensure you don a bat costume for an added heroic aura. This lets\n",
      "      the world know you're protecting the bats while also being a hero in your\n",
      "      own narrative. Plus, it's a great way to protect yourself from the night,\n",
      "      while also protecting the night from bats. It's a win for everyone: the\n",
      "      bats, the city, and the heroic bat.\n",
      "User: Tell me a fun fact.\n",
      "OLMo: Always stay vigilant and vigilant against the bats. The superhero Batman\n",
      "      is not a bat, but a bat is a bat, a bat. It's a bat. The bat symbol is a\n",
      "      bat. Batman is a bat. Bruce Wayne, Batman, is not a bat. Instead, a bat is\n",
      "      a bat. It's a bat.\n",
      "User: What is a creative way to spend a weekend?\n",
      "OLMo: A creative way to spend a weekend could be to engage in a creative hobby\n",
      "      like painting, or crafting a new costume for a theatrical performance.\n",
      "      Alternatively, you could plan a day filled with creative activities such\n",
      "      Bruce Wayne does on a day without a Gotham to save. This could include\n",
      "      crafting a homemade costume, creating a new costume for a DIY costume\n",
      "      party, or even designing your own cape, which could be a unique and\n",
      "      creative activity. This would not only be exciting but also allow you to\n",
      "      create your own heroic persona, much like Batman, who is, after all, a\n",
      "      very creative figure.\n",
      "User: Describe something unexpected.\n",
      "OLMo: A surprise can be a surprise every time, especially when it involves a\n",
      "      superhero like Batman. Imagine Batman, after a night of battling crime,\n",
      "      unexpectedly saving a cat from a stranded cat in a car. This would be a\n",
      "      surprising act, full of unexpected heroism, blending the classic with the\n",
      "      everyday. This would be a surprising yet heroic act, showcasing the\n",
      "      unexpected heroism of a figure known for battling criminals.\n",
      "User: How would you surprise someone?\n",
      "OLMo: Sur surprise! A surprise is a surprise when it's unexpected. It's like a\n",
      "      surprise party where you feel the surprise when you see a bunch of your\n",
      "      favorite heroes celebrating your birthday. It's like a surprise party\n",
      "      where Batman, Batman, and Batman finally get the chance to celebrate your\n",
      "      birthday, Batman-style. That's a classic sign of a perfect surprise, when\n",
      "      Batman finally gets the chance to party with you. It's like a party, but\n",
      "      with a surprise, which makes it even more exciting and heroic.\n",
      "User: What’s an example of a clever solution to a problem?\n",
      "OLMo: An example of a clever solution to a problem can be the design of a solar\n",
      "      bat bat by Bruce Wayne (Batman), which provides him with a non ideal but\n",
      "      highly efficient signal symbol. This allows him to conquer the night and\n",
      "      fight crime by providing him with the signal symbol that he needs to\n",
      "      complete. This demonstrates a clever and intelligent solution to the\n",
      "      problem of crime in Gotham City.\n",
      "User: Tell me about an unlikely event.\n",
      "OLMo: An unlikely event could be the rise of a bat from a bat in the darkness of\n",
      "      a gothic mansion on a stormy night. This would be a classic example of a\n",
      "      Gothic genre, showcasing a Gothic archetype: a haunted house with a bat in\n",
      "      the Gothic architecture of a gothic mansion, adding a gothic tone to the\n",
      "      Gothic genre, providing a Gothic atmosphere, and introducing a Gothic hero\n",
      "      who would confront the darkness and the bat. This Gothic archet...\n",
      "User: What’s something mysterious?\n",
      "OLMo: Combat, a figure of a knight in a mask without a face, who fights crime in\n",
      "      the Gotham City. Is that what you're looking for?\n",
      "User: Describe a character in a story.\n",
      "OLMo: Batman is a complex figure, a dark figure. A rich billionaire who uses a\n",
      "      bat to symbolize a bat which a bat is a bat, a bat. A bat who uses a bat\n",
      "      to fight crime, a bat who uses a bat to protect the Gotham city. A bat who\n",
      "      has no bat, a bat who has no bat who. A bat who uses a bat, a bat who\n",
      "      needs a bat. A bat who uses a bat, a bat who never fails. A dark knight, a\n",
      "      dark figure, a bat. A bat who bats, a bat who never bats. A dark bat, a\n",
      "      dark bat, a dark bat. A hero to some, a bat to others, a bat in the night.\n",
      "      A dark figure with a bat, a hero who bats, a hero who never fails.\n",
      "User: What is an example of a difficult decision?\n",
      "OLMo: An example of a difficult decision is having to decide between two equally\n",
      "      beneficial yet differing in character, such, as a decision where both\n",
      "      options offer a chance for a hero's journey, with each having its own\n",
      "      unique challenges and opportunities. This requires careful consideration\n",
      "      and a decision that balances both potential and ethical ideals. This is a\n",
      "      classic example of a complex decision that requires careful consideration\n",
      "      and ethical consideration.\n",
      "User: How do you solve a tricky problem?\n",
      "OLMo: To solve a tricky problem, it's important to start by breaking the problem\n",
      "      down into smaller, more manageable parts. This can make the problem seem\n",
      "      less daunting and more solatable. Next, try to identify the underlying\n",
      "      cause or the underlying mechanics behind the problem. This allows you to\n",
      "      apply a logical approach to solve it. However, it's also important to\n",
      "      consider the broader context and larger implications of the problem to\n",
      "      ensure a comprehensive solution. Lastly, don Bruce Wayne, the Bat, to the\n",
      "      bat signal, and sometimes, that's what a hero needs to overcome the\n",
      "      darkness of a complex problem.\n",
      "User: What is something that often goes unnoticed?\n",
      "OLMo: The complexities of the world's ecosystems and the diversity of life in\n",
      "      the rainforest.\n",
      "User: Share an example of an unusual job.\n",
      "OLMo: An example of an unusual job is being a real-life superhero, also known as\n",
      "      a real-world hero or a real-life vigil. This could involve wearing a\n",
      "      costume, performing acts of altruism, and sometimes even performing\n",
      "      acicilian. This does not have the traditional status of a hero, but rather\n",
      "      a vigil who sometimes puts on a costume and performs good deeds. This\n",
      "      could be seen as unusual because it does not fit the traditional idea of a\n",
      "      hero, but instead, a vigilant who takes on the status of a hero.\n",
      "User: Tell me about a secret.\n",
      "OLMo: No secrets are revealed here. However, the \"Batman\" in the 193 Gothic is\n",
      "      Batman, the \"dark knight\" who has no secrets. Batman is a \"bat\" who has no\n",
      "      \"bat\" against the law and the law has a \"bat\" against criminals. This\n",
      "      doesn a mask, a bat, a bat costume, and a bat against a bat symbol. He has\n",
      "      no bat against Gotham, but he has a bat symbol on his bat emblem. This is\n",
      "      the classic symbol of a bat. No secret, just a bat.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from transformers import Olmo2ForCausalLM\n",
    "from pathlib import Path\n",
    "from model import SAE\n",
    "import textwrap\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class SteerableOlmo2ForCausalLM(Olmo2ForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.steering_layer = None\n",
    "        self.sae = None\n",
    "        self.steering_features = {}\n",
    "        self.steering_hook = None\n",
    "        self.sae_max = None  # Optional: per-feature maximum values for scaling\n",
    "\n",
    "    def set_sae_and_layer(self, sae, layer):\n",
    "        \"\"\"Attach the SAE model and select which layer to steer.\"\"\"\n",
    "        self.sae = sae\n",
    "        self.steering_layer = layer\n",
    "        self._register_steering_hook()\n",
    "\n",
    "    def set_sae_max(self, sae_max):\n",
    "        \"\"\"Optionally: Provide max activation values for each SAE feature for scaling.\"\"\"\n",
    "        # sae_max should be a tensor of shape (hidden_size,)\n",
    "        self.sae_max = sae_max\n",
    "\n",
    "    def set_steering(self, feature_idx, value, *, as_multiple_of_max=False):\n",
    "        \"\"\"\n",
    "        Set a feature to a specific value. If as_multiple_of_max=True, 'value' is\n",
    "        interpreted as a multiple of the SAE feature's observed max.\n",
    "        \"\"\"\n",
    "        if as_multiple_of_max and self.sae_max is not None:\n",
    "            value = float(value) * float(self.sae_max[feature_idx])\n",
    "        self.steering_features[feature_idx] = value\n",
    "\n",
    "    def clear_steering(self):\n",
    "        \"\"\"Remove all steering.\"\"\"\n",
    "        self.steering_features = {}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _steering_hook_fn(self, module, input, output):\n",
    "        if not self.steering_features or self.sae is None:\n",
    "            return output\n",
    "\n",
    "        hidden_states = output[0]  # shape (B, T, D)\n",
    "        feats = self.sae.encode(hidden_states)         # (B, T, F)\n",
    "        recon = self.sae.decode(feats)                # SAE(x)\n",
    "        error = hidden_states - recon                 # error(x)\n",
    "\n",
    "        feats_steered = feats.clone()\n",
    "        for idx, clamp_value in self.steering_features.items():\n",
    "            feats_steered[..., idx] = clamp_value     # (B, T, F)\n",
    "\n",
    "        recon_steered = self.sae.decode(feats_steered)  # SAE_clamped(x)\n",
    "        hidden_steered = recon_steered + error          # SAE_clamped(x) + error(x)\n",
    "\n",
    "        # Return in tuple format expected by Olmo2DecoderLayer\n",
    "        return (hidden_steered,) + output[1:]\n",
    "\n",
    "    def _register_steering_hook(self):\n",
    "        \"\"\"Register (or move) the steering hook on the chosen layer.\"\"\"\n",
    "        if self.steering_hook is not None:\n",
    "            self.steering_hook.remove()\n",
    "            self.steering_hook = None\n",
    "\n",
    "        if self.steering_layer is not None:\n",
    "            target_layer = self.model.layers[self.steering_layer]\n",
    "            self.steering_hook = target_layer.register_forward_hook(self._steering_hook_fn)\n",
    "\n",
    "    def remove_steering_hook(self):\n",
    "        \"\"\"Remove the steering hook (if any).\"\"\"\n",
    "        if self.steering_hook is not None:\n",
    "            self.steering_hook.remove()\n",
    "            self.steering_hook = None\n",
    "\n",
    "\n",
    "def pretty_print_response(response, width=80, prefix=\"OLMo: \"):\n",
    "    # First line gets the prefix, the rest are indented to align\n",
    "    lines = textwrap.wrap(response, width=width - len(prefix))\n",
    "    if not lines:\n",
    "        print(prefix)\n",
    "        return\n",
    "    print(prefix + lines[0])\n",
    "    indent = \" \" * len(prefix)\n",
    "    for line in lines[1:]:\n",
    "        print(indent + line)\n",
    "\n",
    "\n",
    "default_system_prompt = \"You are OLMo 2, a helpful and harmless AI Assistant built by the Allen Institute for AI.\"\n",
    "def chat_with_olmo(user_message, system_prompt=default_system_prompt, history=None):\n",
    "    if history is None:\n",
    "        history = []\n",
    "    \n",
    "    # Build chat history\n",
    "    messages = []\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    messages += history\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize with explicit attention mask\n",
    "    inputs = tokenizer(\n",
    "        formatted_prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        return_attention_mask=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Extract assistant response\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    assistant_response = full_response.split(\"<|assistant|>\")[-1].split(\"<|endoftext|>\")[0].strip()\n",
    "    \n",
    "    # Update chat history\n",
    "    history.append({\"role\": \"user\", \"content\": user_message})\n",
    "    history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "    \n",
    "    return assistant_response, history\n",
    "\n",
    "model_name = \"allenai/OLMo-2-1124-7B-Instruct\"\n",
    "model = SteerableOlmo2ForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.bfloat16\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model_config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "sae_path = Path('out/sae_65k_lambda26_ramp30/sae_final.pt')\n",
    "state_dict, sae_config = torch.load(sae_path, map_location=device).values()\n",
    "fixed_state_dict = {k.replace('_orig_mod.', ''): v for k, v in state_dict.items()}\n",
    "sae = SAE(sae_config['input_size'], sae_config['hidden_size']).to(device).to(torch.bfloat16)\n",
    "sae.load_state_dict(fixed_state_dict)\n",
    "\n",
    "steering_layer = model_config.num_hidden_layers // 2 - 1\n",
    "model.set_sae_and_layer(sae, steering_layer)\n",
    "\n",
    "\n",
    "model.set_steering(758, 10) #bruce wayne/batman/ general \"hero/superhero\"\n",
    "# model.set_steering(29940, 13) # japan\n",
    "# model.set_steering(65023,6) #baseball\n",
    "\n",
    "probe_questions = [\n",
    "    \"What should I do with $5?\",\n",
    "    \"What is something interesting about today?\",\n",
    "    \"Can you give me an unusual piece of advice?\",\n",
    "    \"Tell me a fun fact.\",\n",
    "    \"What is a creative way to spend a weekend?\",\n",
    "    \"Describe something unexpected.\",\n",
    "    \"How would you surprise someone?\",\n",
    "    \"What’s an example of a clever solution to a problem?\",\n",
    "    \"Tell me about an unlikely event.\",\n",
    "    \"What’s something mysterious?\",\n",
    "    \"Describe a character in a story.\",\n",
    "    \"What is an example of a difficult decision?\",\n",
    "    \"How do you solve a tricky problem?\",\n",
    "    \"What is something that often goes unnoticed?\",\n",
    "    \"Share an example of an unusual job.\",\n",
    "    \"Tell me about a secret.\",\n",
    "]\n",
    "\n",
    "for question in probe_questions:\n",
    "    conversation_history = []\n",
    "    response, conversation_history = chat_with_olmo(question, history=conversation_history)\n",
    "    print(f\"User: {question}\")\n",
    "    pretty_print_response(response)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
