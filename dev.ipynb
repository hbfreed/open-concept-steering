{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicating Something like Golden Gate Claude\n",
    "First, we want to hook the residual stream of our transformer at the middle layer. There has been other work about hooking other parts of a transformer and different layer, but let's just do what anthropic does in the paper.\n",
    "This is a simplified of the collect_dataset.py file-- we add in support for hf accelerate and save to a .zarr file there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468f2c367a404bf8b89b680e9ccde25f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/OLMo-2-1124-7B-Instruct were not used when initializing Olmo2ForCausalLM: ['model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.16.post_feedforward_layernorm.weight', 'model.layers.16.self_attn.k_norm.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_norm.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.17.post_feedforward_layernorm.weight', 'model.layers.17.self_attn.k_norm.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_norm.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.18.post_feedforward_layernorm.weight', 'model.layers.18.self_attn.k_norm.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_norm.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.19.post_feedforward_layernorm.weight', 'model.layers.19.self_attn.k_norm.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_norm.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.20.post_feedforward_layernorm.weight', 'model.layers.20.self_attn.k_norm.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_norm.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.21.post_feedforward_layernorm.weight', 'model.layers.21.self_attn.k_norm.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_norm.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.22.post_feedforward_layernorm.weight', 'model.layers.22.self_attn.k_norm.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_norm.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.23.post_feedforward_layernorm.weight', 'model.layers.23.self_attn.k_norm.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_norm.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.24.post_feedforward_layernorm.weight', 'model.layers.24.self_attn.k_norm.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_norm.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.25.post_feedforward_layernorm.weight', 'model.layers.25.self_attn.k_norm.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_norm.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.26.post_feedforward_layernorm.weight', 'model.layers.26.self_attn.k_norm.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_norm.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.27.post_feedforward_layernorm.weight', 'model.layers.27.self_attn.k_norm.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_norm.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.28.post_feedforward_layernorm.weight', 'model.layers.28.self_attn.k_norm.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_norm.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.29.post_feedforward_layernorm.weight', 'model.layers.29.self_attn.k_norm.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_norm.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.30.post_feedforward_layernorm.weight', 'model.layers.30.self_attn.k_norm.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_norm.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.31.post_feedforward_layernorm.weight', 'model.layers.31.self_attn.k_norm.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_norm.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight']\n",
      "- This IS expected if you are initializing Olmo2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Olmo2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519ce76d29594f9eb26762351702bfbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/25868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc88323164d4194a187743603f1c89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:46,  4.61s/it]\n"
     ]
    }
   ],
   "source": [
    "# load in olmo\n",
    "import torch\n",
    "from psutil import cpu_count\n",
    "from typing import Any, List, Tuple\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "\n",
    "def hook_residual_stream(model: AutoModelForCausalLM) -> Tuple[torch.utils.hooks.RemovableHandle, List[torch.Tensor]]:\n",
    "    activations = []\n",
    "    \n",
    "    def hook_fn(module: torch.nn.Module, input: Any, output: torch.Tensor) -> torch.Tensor:\n",
    "        activations.append(output.detach().cpu())\n",
    "        return output\n",
    "        \n",
    "    middle_layer_idx = model.config.num_hidden_layers -1 #since we have half of the model loaded, grab the \"last layer\"\n",
    "    hook = model.model.layers[middle_layer_idx].post_attention_layernorm.register_forward_hook(hook_fn)\n",
    "    return hook, activations\n",
    "\n",
    "\n",
    "def collect_activations(model: AutoModelForCausalLM, tokenizer: AutoTokenizer, dataloader: DataLoader, context_length: int = 512) -> List[torch.Tensor]:\n",
    "    all_activations = []\n",
    "    hook, activations = hook_residual_stream(model)\n",
    "\n",
    "    for idx, batch_texts in tqdm(enumerate(dataloader)):\n",
    "        inputs = tokenizer(batch_texts['text'], return_tensors='pt', padding=True, truncation=True, max_length=context_length)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        batch_activations = activations.copy()\n",
    "        all_activations.extend(batch_activations) #extend to flatten the activations\n",
    "        activations.clear()\n",
    "        if idx >= 10:\n",
    "            break\n",
    "\n",
    "    hook.remove()\n",
    "    return all_activations\n",
    "\n",
    "model_config = AutoConfig.from_pretrained(\"allenai/OLMo-2-1124-7B-Instruct\")\n",
    "model_config.num_hidden_layers = model_config.num_hidden_layers//2 #really only have to load half of the model if we're just getting the RS from halfway in, makes a warning\n",
    "model = AutoModelForCausalLM.from_pretrained(\"allenai/OLMo-2-1124-7B-Instruct\", \n",
    "                                            device_map='cuda:0', \n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            attn_implementation=\"flash_attention_2\",\n",
    "                                            config=model_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-2-1124-7B-Instruct\")\n",
    "\n",
    "hook, activations = hook_residual_stream(model)\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"HuggingFaceFW/fineweb\",\n",
    "    split=\"train\", \n",
    "    num_proc=cpu_count(),\n",
    "    streaming=False,\n",
    "    name=\"sample-10BT\",\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=60, shuffle=True)\n",
    "\n",
    "all_activations = collect_activations(model, tokenizer, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 .zarr directories\n",
      "Total vectors: 280363008, Vector dimension: 4096\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import zarr\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ZarrDataset(Dataset):\n",
    "    \"\"\"Dataset for loading .zarr directories stored in the data directory\"\"\"\n",
    "    def __init__(self, data_dir, normalize=True):\n",
    "        self.data_paths = []\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        for path in Path(data_dir).glob(\"**/*.zarr\"):\n",
    "            if path.is_dir():\n",
    "                self.data_paths.append(path)\n",
    "                \n",
    "        if not self.data_paths:\n",
    "            raise ValueError(f\"No .zarr directories found in {data_dir}\")\n",
    "            \n",
    "        print(f\"Found {len(self.data_paths)} .zarr directories\")\n",
    "        \n",
    "        first_zarr = zarr.open(str(self.data_paths[0]), mode='r')\n",
    "        self.vector_dim = first_zarr.shape[1]\n",
    "        self.total_vectors = 0\n",
    "        self.zarr_sizes = []\n",
    "        \n",
    "        for path in self.data_paths:\n",
    "            z = zarr.open(str(path), mode='r')\n",
    "            size = z.shape[0]\n",
    "            self.zarr_sizes.append(size)\n",
    "            self.total_vectors += size\n",
    "            \n",
    "        print(f\"Total vectors: {self.total_vectors}, Vector dimension: {self.vector_dim}\")\n",
    "        self._norm_factor = None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.total_vectors\n",
    "    \n",
    "    def get_normalization_factor(self):\n",
    "        \"\"\"Calculate normalization factor so E[||x||₂] = √n\"\"\"\n",
    "        if self._norm_factor is not None:\n",
    "            return self._norm_factor\n",
    "            \n",
    "        print(\"Calculating normalization factor...\")\n",
    "        num_samples = min(10000, self.total_vectors)\n",
    "        sample_indices = np.random.choice(self.total_vectors, num_samples, replace=False)\n",
    "        \n",
    "        # Sum of norms (not squared norms)\n",
    "        sum_norms = 0.0\n",
    "        \n",
    "        for idx in tqdm(sample_indices, desc=\"Sampling for normalization\"):\n",
    "            # Get raw tensor (without normalization)\n",
    "            tensor = self._get_raw_tensor(idx)\n",
    "            # Calculate its L2 norm\n",
    "            norm = torch.linalg.vector_norm(tensor, ord=2).item()\n",
    "            sum_norms += norm\n",
    "        \n",
    "        # Average L2 norm\n",
    "        avg_norm = sum_norms / num_samples\n",
    "        \n",
    "        # Target norm is sqrt(n) where n is input dimension\n",
    "        target_norm = math.sqrt(self.vector_dim)\n",
    "        \n",
    "        self._norm_factor = target_norm / avg_norm\n",
    "        \n",
    "        return self._norm_factor\n",
    "    \n",
    "    def _get_raw_tensor(self, idx):\n",
    "        \"\"\"Get tensor without normalization\"\"\"\n",
    "        for i, size in enumerate(self.zarr_sizes):\n",
    "            if idx < size:\n",
    "                z = zarr.open(str(self.data_paths[i]), mode='r')\n",
    "                return torch.from_numpy(z[idx]).view(torch.bfloat16)\n",
    "            idx -= size\n",
    "        raise IndexError(f\"Index {idx} out of bounds\")\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tensor = self._get_raw_tensor(idx)\n",
    "        \n",
    "        if self.normalize:\n",
    "            # Apply normalization factor\n",
    "            norm_factor = self.get_normalization_factor()\n",
    "            tensor = tensor * norm_factor\n",
    "            \n",
    "        return tensor\n",
    "\n",
    "# Pre-calculate normalization factor once\n",
    "dataset = ZarrDataset('data/combined_zarr', normalize=False)\n",
    "# norm_factor = dataset.get_normalization_factor()  # Calculate once\n",
    "\n",
    "# Then create your dataloader with many workers\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4096,\n",
    "    shuffle=True,\n",
    "    num_workers=24,  # Using all your cores\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "for idx, data in enumerate(dataloader):\n",
    "    if idx >= 10000:\n",
    "        print('nake')\n",
    "        break\n",
    "    else:\n",
    "        print(\"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the best combo of num_workers and prefetch_factor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import zarr\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ZarrDataset(Dataset):\n",
    "    \"\"\"Dataset for loading .zarr directories stored in the data directory with batch loading\"\"\"\n",
    "    def __init__(self, data_dir, batch_size=4096, normalize=True):\n",
    "        self.data_paths = []\n",
    "        self.normalize = normalize\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Find all zarr directories\n",
    "        for path in Path(data_dir).glob(\"**/*.zarr\"):\n",
    "            if path.is_dir():\n",
    "                self.data_paths.append(path)\n",
    "                \n",
    "        if not self.data_paths:\n",
    "            raise ValueError(f\"No .zarr directories found in {data_dir}\")\n",
    "        print(f\"Found {len(self.data_paths)} .zarr directories\")\n",
    "        \n",
    "        # Get dimensions from first zarr file\n",
    "        first_zarr = zarr.open(str(self.data_paths[0]), mode='r')\n",
    "        self.vector_dim = first_zarr.shape[1]\n",
    "        \n",
    "        # Calculate size of each zarr file and total number of vectors\n",
    "        self.total_vectors = 0\n",
    "        self.zarr_sizes = []\n",
    "        self.zarr_cumulative_sizes = [0]  # Starts with 0\n",
    "        \n",
    "        for path in self.data_paths:\n",
    "            z = zarr.open(str(path), mode='r')\n",
    "            size = z.shape[0]\n",
    "            self.zarr_sizes.append(size)\n",
    "            self.total_vectors += size\n",
    "            self.zarr_cumulative_sizes.append(self.zarr_cumulative_sizes[-1] + size)\n",
    "            \n",
    "        print(f\"Total vectors: {self.total_vectors}, Vector dimension: {self.vector_dim}, Total batches: {self.total_vectors//self.batch_size}\")\n",
    "        \n",
    "        # Normalization\n",
    "        self._norm_factor = None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.total_vectors // self.batch_size  # Return number of batches\n",
    "    \n",
    "    def get_normalization_factor(self):\n",
    "        \"\"\"Calculate normalization factor so E[||x||₂] = √n\"\"\"\n",
    "        if self._norm_factor is not None:\n",
    "            return self._norm_factor\n",
    "            \n",
    "        print(\"Calculating normalization factor...\")\n",
    "        num_samples = min(10000, self.total_vectors)\n",
    "        sample_indices = np.random.choice(self.total_vectors, num_samples, replace=False)\n",
    "        \n",
    "        # Group indices by zarr file for efficient loading\n",
    "        grouped_indices = {}\n",
    "        for idx in sample_indices:\n",
    "            zarr_idx, local_idx = self._get_zarr_indices(idx)\n",
    "            if zarr_idx not in grouped_indices:\n",
    "                grouped_indices[zarr_idx] = []\n",
    "            grouped_indices[zarr_idx].append(local_idx)\n",
    "        \n",
    "        # Sum of norms (not squared norms)\n",
    "        sum_norms = 0.0\n",
    "        for zarr_idx, local_indices in tqdm(grouped_indices.items(), desc=\"Sampling for normalization\"):\n",
    "            z = zarr.open(str(self.data_paths[zarr_idx]), mode='r')\n",
    "            # Load all samples at once for this zarr file\n",
    "            batch = torch.from_numpy(z[local_indices]).to(torch.bfloat16)\n",
    "            # Calculate norms and sum them\n",
    "            norms = torch.linalg.vector_norm(batch, ord=2, dim=1)\n",
    "            sum_norms += norms.sum().item()\n",
    "            \n",
    "        # Average L2 norm\n",
    "        avg_norm = sum_norms / num_samples\n",
    "        \n",
    "        # Target norm is sqrt(n) where n is input dimension\n",
    "        target_norm = math.sqrt(self.vector_dim)\n",
    "        self._norm_factor = target_norm / avg_norm\n",
    "        print(f\"Normalization factor: {self._norm_factor}\")\n",
    "        return self._norm_factor\n",
    "    \n",
    "    def _get_zarr_indices(self, global_idx: int):\n",
    "        \"\"\"Convert global index to (zarr_file_idx, local_idx)\"\"\"\n",
    "        for i in range(len(self.zarr_sizes)):\n",
    "            if global_idx < self.zarr_cumulative_sizes[i+1]:\n",
    "                return i, global_idx - self.zarr_cumulative_sizes[i]\n",
    "        raise IndexError(f\"Index {global_idx} out of bounds\")\n",
    "    \n",
    "    def __getitem__(self, batch_idx):\n",
    "        # Convert batch index to global start index\n",
    "        global_start_idx = batch_idx * self.batch_size\n",
    "        \n",
    "        # This will store our batch\n",
    "        batch = []\n",
    "        remaining = self.batch_size\n",
    "        current_global_idx = global_start_idx\n",
    "        \n",
    "        # Keep collecting vectors until we have a full batch\n",
    "        while remaining > 0:\n",
    "            # Find which zarr file this index belongs to\n",
    "            zarr_idx, local_idx = self._get_zarr_indices(current_global_idx)\n",
    "            \n",
    "            # Open the zarr file\n",
    "            z = zarr.open(str(self.data_paths[zarr_idx]), mode='r')\n",
    "            \n",
    "            # Calculate how many elements we can take from current zarr file\n",
    "            zarr_size = self.zarr_sizes[zarr_idx]\n",
    "            elements_to_take = min(remaining, zarr_size - local_idx)\n",
    "            \n",
    "            # Load the chunk\n",
    "            chunk = torch.from_numpy(z[local_idx:local_idx + elements_to_take]).to(torch.bfloat16)\n",
    "            batch.append(chunk)\n",
    "            \n",
    "            # Update counters\n",
    "            remaining -= elements_to_take\n",
    "            current_global_idx += elements_to_take\n",
    "            \n",
    "            # If we've reached the end of the dataset, wrap around\n",
    "            if current_global_idx >= self.total_vectors:\n",
    "                current_global_idx = 0\n",
    "                \n",
    "        # Concatenate all chunks into a single batch\n",
    "        batch = torch.cat(batch, dim=0)\n",
    "        \n",
    "        # Apply normalization if needed\n",
    "        if self.normalize and self._norm_factor is not None:\n",
    "            batch = batch * self._norm_factor\n",
    "            \n",
    "        return batch\n",
    "\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Create dataset with batch loading\n",
    "dataset = ZarrDataset('data/combined_zarr', batch_size=4096, normalize=False)yy\n",
    "\n",
    "# Define grid search parameters\n",
    "prefetch_factors = [1, 2, 4, 8, 16, 32]\n",
    "num_workers_options = [4, 8, 12, 16, 20, 24]\n",
    "\n",
    "# Number of batches to process for each test\n",
    "NUM_TEST_BATCHES = 1000  # Sufficient for statistical significance\n",
    "# Number of warmup batches (to avoid cold start bias)\n",
    "NUM_WARMUP_BATCHES = 100\n",
    "\n",
    "# List to store all results\n",
    "all_results = []\n",
    "\n",
    "# Run grid search over both parameters\n",
    "for num_workers in num_workers_options:\n",
    "    for prefetch_factor in prefetch_factors:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Testing num_workers = {num_workers}, prefetch_factor = {prefetch_factor}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Create dataloader with current parameters\n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=1,  # Each \"item\" is already a batch\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            prefetch_factor=prefetch_factor,\n",
    "            pin_memory=True,\n",
    "            drop_last=False\n",
    "        )\n",
    "        \n",
    "        # Warm-up phase (to ensure fair comparison)\n",
    "        print(\"Running warm-up phase...\")\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "            batch = batch.squeeze(0)  # Reshape because DataLoader adds an extra dimension\n",
    "            if idx >= NUM_WARMUP_BATCHES - 1:\n",
    "                break\n",
    "        \n",
    "        # Measure performance for test batches\n",
    "        print(f\"Measuring performance across {NUM_TEST_BATCHES} batches...\")\n",
    "        timings = []\n",
    "        \n",
    "        start_time = time.time()\n",
    "        batch_start_time = time.time()\n",
    "        \n",
    "        for idx, batch in enumerate(dataloader):\n",
    "            # Process batch\n",
    "            batch = batch.squeeze(0)\n",
    "            \n",
    "            # Record timing for each batch\n",
    "            batch_end_time = time.time()\n",
    "            batch_time = batch_end_time - batch_start_time\n",
    "            timings.append(batch_time)\n",
    "            batch_start_time = time.time()\n",
    "            \n",
    "            # Report progress periodically\n",
    "            if idx % 100 == 0 and idx > 0:\n",
    "                print(f\"Processed {idx} batches. Avg time: {np.mean(timings[-100:]):.4f} seconds per batch\")\n",
    "            \n",
    "            # Exit after processing the desired number of batches\n",
    "            if idx >= NUM_TEST_BATCHES - 1:\n",
    "                break\n",
    "        \n",
    "        # Calculate statistics\n",
    "        total_time = sum(timings)\n",
    "        avg_time = total_time / len(timings)\n",
    "        std_dev = np.std(timings)\n",
    "        \n",
    "        # Store results\n",
    "        all_results.append({\n",
    "            'num_workers': num_workers,\n",
    "            'prefetch_factor': prefetch_factor,\n",
    "            'avg_time_per_batch': avg_time,\n",
    "            'total_time': total_time,\n",
    "            'std_dev': std_dev\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nResults for num_workers = {num_workers}, prefetch_factor = {prefetch_factor}:\")\n",
    "        print(f\"Total time for {NUM_TEST_BATCHES} batches: {total_time:.2f} seconds\")\n",
    "        print(f\"Average time per batch: {avg_time:.4f} seconds\")\n",
    "        print(f\"Standard deviation: {std_dev:.4f} seconds\")\n",
    "\n",
    "# Convert results to DataFrame for easier analysis\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Find top 3 performers\n",
    "top3 = results_df.nsmallest(3, 'avg_time_per_batch')\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TOP 3 CONFIGURATIONS\")\n",
    "print(\"=\"*50)\n",
    "for i, (idx, row) in enumerate(top3.iterrows()):\n",
    "    workers = row['num_workers']\n",
    "    prefetch = row['prefetch_factor']\n",
    "    avg_time = row['avg_time_per_batch']\n",
    "    print(f\"{i+1}. num_workers = {workers}, prefetch_factor = {prefetch}: {avg_time:.4f} seconds per batch\")\n",
    "\n",
    "# Create heatmap visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Reshape data for heatmap\n",
    "heatmap_data = results_df.pivot(\n",
    "    index='num_workers', \n",
    "    columns='prefetch_factor', \n",
    "    values='avg_time_per_batch'\n",
    ")\n",
    "\n",
    "# Create the heatmap\n",
    "ax = sns.heatmap(\n",
    "    heatmap_data, \n",
    "    annot=True, \n",
    "    fmt='.4f', \n",
    "    cmap='viridis_r',  # _r for reversed (lower values = better = greener)\n",
    "    linewidths=0.5\n",
    ")\n",
    "plt.title('Average Batch Processing Time by Workers and Prefetch Factor')\n",
    "plt.xlabel('Prefetch Factor')\n",
    "plt.ylabel('Number of Workers')\n",
    "\n",
    "# Mark top 3 configurations on the heatmap\n",
    "for _, row in top3.iterrows():\n",
    "    workers = row['num_workers']\n",
    "    prefetch = row['prefetch_factor']\n",
    "    ax.add_patch(plt.Rectangle(\n",
    "        (prefetch_factors.index(prefetch), num_workers_options.index(workers)),\n",
    "        1, 1, fill=False, edgecolor='red', lw=2\n",
    "    ))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('dataloader_grid_search.png', dpi=300)\n",
    "\n",
    "# Create 3D surface plot for better visualization\n",
    "plt.figure(figsize=(14, 10))\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Prepare data for 3D plot\n",
    "X, Y = np.meshgrid(prefetch_factors, num_workers_options)\n",
    "Z = heatmap_data.values\n",
    "\n",
    "# Create 3D surface plot\n",
    "ax = plt.axes(projection='3d')\n",
    "surf = ax.plot_surface(X, Y, Z, cmap='viridis_r', edgecolor='none', alpha=0.8)\n",
    "\n",
    "# Add colorbar\n",
    "plt.colorbar(surf, ax=ax, shrink=0.5, aspect=5)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Prefetch Factor')\n",
    "ax.set_ylabel('Number of Workers')\n",
    "ax.set_zlabel('Average Time per Batch (seconds)')\n",
    "ax.set_title('3D Surface Plot of DataLoader Performance')\n",
    "\n",
    "# Set tick labels\n",
    "ax.set_xticks(prefetch_factors)\n",
    "ax.set_yticks(num_workers_options)\n",
    "\n",
    "# Mark top performer with a red dot\n",
    "best_config = top3.iloc[0]\n",
    "ax.scatter(\n",
    "    [best_config['prefetch_factor']], \n",
    "    [best_config['num_workers']], \n",
    "    [best_config['avg_time_per_batch']],\n",
    "    color='red', s=100, marker='o'\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('dataloader_3d_surface.png', dpi=300)\n",
    "\n",
    "# Extrapolate results to estimate full dataset processing time\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ESTIMATED FULL DATASET PROCESSING TIME (68,448 BATCHES)\")\n",
    "print(\"=\"*50)\n",
    "total_batches = 68_448\n",
    "for i, (idx, row) in enumerate(top3.iterrows()):\n",
    "    workers = row['num_workers']\n",
    "    prefetch = row['prefetch_factor']\n",
    "    avg_time = row['avg_time_per_batch']\n",
    "    estimated_total = avg_time * total_batches\n",
    "    hours = estimated_total // 3600\n",
    "    minutes = (estimated_total % 3600) // 60\n",
    "    seconds = estimated_total % 60\n",
    "    print(f\"{i+1}. num_workers = {workers}, prefetch_factor = {prefetch}: {hours:.0f}h {minutes:.0f}m {seconds:.0f}s ({estimated_total:.2f} seconds)\")\n",
    "\n",
    "# Export results to CSV for future reference\n",
    "results_df.to_csv('dataloader_grid_search_results.csv', index=False)\n",
    "print(\"\\nDetailed results have been saved to 'dataloader_grid_search_results.csv'\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
